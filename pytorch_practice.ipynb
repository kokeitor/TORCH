{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JVQ13l32QpB",
        "outputId": "79961510-ee34-4c42-8373-8bfb55d03ed4"
      },
      "outputs": [],
      "source": [
        "# Esto es solo para poder debugear.\n",
        "#!pip install torch tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "# Genera una semilla fija para que los experimentos sea repetibles.\n",
        "t_cg = torch.manual_seed(1547)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MbJDaOcZGIr"
      },
      "source": [
        "### NUMPY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k40zE64ZUegJ"
      },
      "outputs": [],
      "source": [
        "x = np.array([[1, 2,3, 4]],dtype = np.float32)\n",
        "y = np.array([2, 4,6,8], dtype = np.float32)\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n",
        "def loss(y_pred, y):\n",
        "  return np.mean(((y_pred - y)**2))\n",
        "\n",
        "def grad(y_pred,y):\n",
        "  return (x* 2 * ((y_pred) - y)).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5ayDy4JU3Q9",
        "outputId": "a608d9bd-16f8-47aa-af7d-dd44502c2683"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30.0 1.5\n",
            "1.875 1.875\n",
            "0.1171875 1.96875\n",
            "0.0073242188 1.9921875\n",
            "0.00045776367 1.998046875\n",
            "2.861023e-05 1.99951171875\n",
            "1.7881393e-06 1.9998779296875\n",
            "1.1175871e-07 1.999969482421875\n",
            "6.9849193e-09 1.9999923706054688\n",
            "4.3655746e-10 1.9999980926513672\n",
            "9.999990463256836\n"
          ]
        }
      ],
      "source": [
        "w= 0.0\n",
        "lr =0.05\n",
        "for e in range(10):\n",
        "  y_pred = forward(x)\n",
        "  loss_i = loss(y_pred,y)\n",
        "  grad_i = grad(y_pred,y)\n",
        "  w -= lr*grad_i\n",
        "  print(loss_i, w)\n",
        "\n",
        "print(forward(x = 5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1RouJqWZHQ3"
      },
      "source": [
        "### TORCH METHOD 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZEedSeCoZFT4"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1,2,3, 4],dtype = torch.float32, requires_grad = False)\n",
        "y = torch.tensor([2,4,6,8], dtype = torch.float32, requires_grad = False)\n",
        "w = torch.tensor(0.0,dtype = torch.float32, requires_grad = True)\n",
        "\n",
        "\n",
        "def forward(x):\n",
        "  return w *  x\n",
        "def loss(y_pred, y):\n",
        "  return ((y_pred - y)**2).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPV0BhWwhbaS",
        "outputId": "9f3b845e-e48e-4a47-ee54-adad8f82c58e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 1786.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 30.000\n",
            "w : 0.300\n",
            "epoch : 10\n",
            "loss : 1.163\n",
            "w : 1.665\n",
            "epoch : 20\n",
            "loss : 0.045\n",
            "w : 1.934\n",
            "epoch : 30\n",
            "loss : 0.002\n",
            "w : 1.987\n",
            "epoch : 40\n",
            "loss : 0.000\n",
            "w : 1.997\n",
            "9.997042655944824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "lr =0.01\n",
        "for e in tqdm.tqdm(range(50)):\n",
        "  y_pred = forward(x)\n",
        "  l  = loss(y_pred, y)\n",
        "  l.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    w -= lr * w.grad\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if e % 10 == 0:\n",
        "    print(f\"epoch : {e}\")\n",
        "    print(f\"loss : {l.item():.3f}\")\n",
        "    print(f\"w : {w.item():.3f}\")\n",
        "\n",
        "print(forward(x = 5).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBUak5v2gJYp"
      },
      "source": [
        "### TORCH METHOD 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4OUYDA6mJfMm"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "x = torch.tensor([1,2,3, 4],dtype = torch.float32, requires_grad = False)\n",
        "y = torch.tensor([2,4,6,8], dtype = torch.float32, requires_grad = False)\n",
        "w = torch.tensor(0.0,dtype = torch.float32, requires_grad = True)\n",
        "\n",
        "#otra forma seria : instanciar clase que devuelve objeto MSEloss\n",
        "# loss = nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt8l9pjvPuH4",
        "outputId": "34afe7b2-0c27-406c-dca6-831d92e4aa0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 3845.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 30.000\n",
            "w : 0.300\n",
            "epoch : 10\n",
            "loss : 1.163\n",
            "w : 1.665\n",
            "epoch : 20\n",
            "loss : 0.045\n",
            "w : 1.934\n",
            "epoch : 30\n",
            "loss : 0.002\n",
            "w : 1.987\n",
            "epoch : 40\n",
            "loss : 0.000\n",
            "w : 1.997\n",
            "9.997042655944824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "lr =0.01\n",
        "for e in tqdm.tqdm(range(50)):\n",
        "  y_pred = forward(x)\n",
        "  # simula la funcion de loss implementada a mano antes\n",
        "\n",
        "  # Metodo 1: instanciando clase de error\n",
        "  # l = loss(input = y_pred, target = y)\n",
        "\n",
        "  # Metodo 2 : simulan la funcion de loss implementada a mano antes mediante una funcion de error built in de torch:\n",
        "  l  = nn.functional.mse_loss(input = y_pred, target = y, size_average=None, reduce=None, reduction='mean')\n",
        "  l.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    w -= lr * w.grad\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if e % 10 == 0:\n",
        "    print(f\"epoch : {e}\")\n",
        "    print(f\"loss : {l.item():.3f}\")\n",
        "    print(f\"w : {w.item():.3f}\")\n",
        "\n",
        "print(forward(x = 5).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEYAJ4W9hr5X"
      },
      "source": [
        "### TORCH METHOD 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3yaq1ZBRQIqq"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "x = torch.tensor([1,2,3, 4],dtype = torch.float32, requires_grad = False)\n",
        "y = torch.tensor([2,4,6,8], dtype = torch.float32, requires_grad = False)\n",
        "w = torch.tensor(0.0,dtype = torch.float32, requires_grad = True)\n",
        "\n",
        "# loss class\n",
        "loss = nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
        "\n",
        "# crea optimizer para prescendir d actualizacion de pesos a mano\n",
        "optimizer = torch.optim.SGD(params = [w], lr=0.01)\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZfjHjk2iBTU",
        "outputId": "f645192e-1657-4664-ec98-d50fedc36007"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 543.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 30.000\n",
            "w : 0.300\n",
            "epoch : 10\n",
            "loss : 1.163\n",
            "w : 1.665\n",
            "epoch : 20\n",
            "loss : 0.045\n",
            "w : 1.934\n",
            "epoch : 30\n",
            "loss : 0.002\n",
            "w : 1.987\n",
            "epoch : 40\n",
            "loss : 0.000\n",
            "w : 1.997\n",
            "9.997042655944824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in tqdm.tqdm(range(50)):\n",
        "  # forward\n",
        "  y_pred = forward(x)\n",
        "\n",
        "  # compute loss for all the batch\n",
        "  l   = loss(input = y_pred, target = y)\n",
        "\n",
        "  # compute the gradients for each w_i : dloss/dw_i (usa grafo computacional y almacena ,en el tensor .grad asociado a cada parametro, el valor del gradiente que luego usa el algoritmo de optimizacion)\n",
        "  l.backward()\n",
        "\n",
        "  # paso del optimizer (en este caso un simple SGD) equivale a lo que haciamos a mano de w += -lr*w.grad\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"epoch : {epoch}\")\n",
        "    print(f\"loss : {l.item():.3f}\")\n",
        "    print(f\"w : {w.item():.3f}\")\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "print(forward(x = 5).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eft4lJyOqM9q"
      },
      "source": [
        "### TORCH METHOD 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "BYPG397vqPYO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "x = torch.tensor([[1],[2],[3], [4]],dtype = torch.float32, requires_grad = False)\n",
        "y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32, requires_grad = False)\n",
        "\n",
        "# Dimension layers (in and out, entra 4 samples (batch) sale 1 output por cada input) teniendo en cuenta el batch: X_input(1,4) * W(4,1)--> (1,1) // sin batch : X_input(1) * W(1)--> (1)\n",
        "batch_size , features  = x.shape\n",
        "# features == dim_input y en este caso features ==  dim_output pq capa solo 1 parametro (como que solo una neurona)\n",
        "\n",
        "# model [capa lineal inicializa un tensor paramters asociado a la capa en funcion de las input y output dimesnions especificadas ]: capa lineal con 1 W y 1 X input (escalares) ; luego dim_input = 1 y dim_output = 1\n",
        "model = nn.Linear(in_features= 1, out_features = 1)\n",
        "\n",
        "# loss class\n",
        "loss = nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
        "\n",
        "# crea optimizer para prescendir de actualizacion de pesos a mano\n",
        "optimizer = torch.optim.SGD(params = model.parameters(), lr=0.01)\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrJjVppMsOQC",
        "outputId": "34a42a02-75f1-4df4-9e3b-794be874aec0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 21.203\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Linear' object has no attribute 'get_parameters'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[65], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parameters\u001b[49m()\u001b[38;5;241m.\u001b[39mitems()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(forward(x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[1;32mc:\\Users\\Jorge\\Desktop\\MASTER_IA\\TORCH\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'get_parameters'"
          ]
        }
      ],
      "source": [
        "for epoch in tqdm.tqdm(range(50)):\n",
        "  # forward now is == to calling the model\n",
        "  y_pred = model(x)\n",
        "\n",
        "  # compute loss for all the batch\n",
        "  l   = loss(input = y_pred, target = y)\n",
        "\n",
        "  # compute the gradients for each w_i : dloss/dw_i (usa grafo computacional y almacena ,en el tensor .grad asociado a cada parametro, el valor del gradiente que luego usa el algoritmo de optimizacion)\n",
        "  l.backward()\n",
        "\n",
        "  # paso del optimizer (en este caso un simple SGD) equivale a lo que haciamos a mano de w += -lr*w.grad\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"epoch : {epoch}\")\n",
        "    print(f\"loss : {l.item():.3f}\")\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "print(forward(x = 5).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNpS7kFUkohP"
      },
      "source": [
        "### TORCH METHOD 5  : custom module + dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "znZayzMfktvh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 1) (100,)\n",
            "(100, 1) (100,)\n",
            "torch.Size([100, 1]) torch.Size([100])\n",
            "torch.Size([100, 1]) torch.Size([100, 1])\n",
            "Media de target y : -0.000\n",
            "Media de target y std: -0.000\n",
            "\n",
            "\n",
            "\n",
            "Module : LinearRegression(\n",
            "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Module : Linear(in_features=1, out_features=1, bias=True)\n",
            "\n",
            "Parameter : ('linear.weight', Parameter containing:\n",
            "tensor([[0.9165]], requires_grad=True))\n",
            "\n",
            "Parameter : ('linear.bias', Parameter containing:\n",
            "tensor([-0.5463], requires_grad=True))\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# data samples\n",
        "x_numpy , y_numpy = datasets.make_regression(n_samples =100, n_features= 1, noise =20, random_state = 1)\n",
        "x_numpy_std  = (x_numpy - np.mean(x_numpy)) / np.std(x_numpy)\n",
        "y_numpy_std = (y_numpy - np.mean(y_numpy)) / np.std(y_numpy)\n",
        "print(x_numpy.shape, y_numpy.shape)\n",
        "print(x_numpy_std.shape, y_numpy_std.shape)\n",
        "\n",
        "# to torch tensor\n",
        "x = torch.from_numpy(x_numpy_std.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy_std.astype(np.float32))\n",
        "print(x.shape, y.shape)\n",
        "# reshape y from (100) tensor -> (100,1) tensor\n",
        "y = y.view(y.shape[0],1)\n",
        "print(x.shape, y.shape)\n",
        "print(f\"Media de target y : {y.mean(dim = 0).item():.3f}\")\n",
        "print(f\"Media de target y std: {y.mean(dim = 0).item():.3f}\")\n",
        "\n",
        "# numbeer of samples and number of features\n",
        "# x (100,1)\n",
        "n_samples = x.shape[0]\n",
        "features = x.shape[1]\n",
        "\n",
        "# custom class model (linear regression layer)\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_size, output_size) -> None:\n",
        "       super().__init__()\n",
        "       self.in_size = input_size\n",
        "       self.out_size = output_size\n",
        "       # model [capa lineal inicializa un tensor paramters asociado a la capa en funcion de las input y output dimesnions especificadas ]: capa lineal con 1 W y 1 X input (escalares) ; luego dim_input = 1 y dim_output = 1\n",
        "       self.linear = nn.Linear(in_features= self.in_size, out_features = self.out_size, bias = True)\n",
        "    def forward(self,x):\n",
        "        return self.linear(x)\n",
        "    \n",
        "\n",
        "# instanciar capa y crear modelo\n",
        "model = LinearRegression(input_size = features, output_size = features)\n",
        "# loss class\n",
        "loss = nn.MSELoss(reduction='mean')\n",
        "# crea optimizer para prescendir de actualizacion de pesos a mano\n",
        "optimizer = torch.optim.SGD(params = model.parameters(), lr=0.01)\n",
        "\n",
        "# MODULE CLASS DOCU:\n",
        "\"\"\"nn.Module :\n",
        "Base class for all neural network modules.\n",
        "\n",
        "Your models should also subclass this class.\n",
        "\n",
        "Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes: \n",
        "self.sub_module = nn.Linear(...)\"\"\"\n",
        "\n",
        "# Iterate along all de modules inside a network class or model class. Notice that LinearRegression module has inside a linear layer \"module\" or only linear layer\n",
        "# note that the atribute name : self.linear will define the string \"linear\" to refer to that layer inside a module\n",
        "# this will be useful when ,inside a class module, there are several layers\n",
        "print(\"\\n\")\n",
        "\n",
        "for m in model.modules():\n",
        "    print(f\"\\nModule : {m}\")\n",
        "\n",
        "# iterate along all the parameters inside a module ( a module can have a lot of layers with their parameters)\n",
        "for p in model.named_parameters(prefix='', recurse=True, remove_duplicate=True):\n",
        "    print(f\"\\nParameter : {p}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APdfIuxGQTuu",
        "outputId": "8d679be4-54fe-499b-86be-548f69d2049f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 1515.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 0.360\n",
            "w : 0.918\n",
            "Predictions :tensor([[4.5543],\n",
            "        [5.5050]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "epochs = 50\n",
        "for epoch in tqdm.tqdm(range(epochs)):\n",
        "    \n",
        "  # forward now is == to calling the model\n",
        "  y_pred = model(x)\n",
        "\n",
        "  # compute loss for all the batch\n",
        "  l   = loss(input = y_pred, target = y)\n",
        "\n",
        "  # compute the gradients for each w_i : dloss/dw_i (usa grafo computacional y almacena ,en el tensor .grad asociado a cada parametro, el valor del gradiente que luego usa el algoritmo de optimizacion)\n",
        "  l.backward()\n",
        "\n",
        "  # paso del optimizer (en este caso un simple SGD) equivale a lo que haciamos a mano de w += -lr*w.grad\n",
        "  optimizer.step()\n",
        "  \n",
        "  # restore or eliminate the grads inside parameter.grad tensor for next iteration\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    print(f\"epoch : {epoch}\")\n",
        "    print(f\"loss : {l.item():.3f}\")\n",
        "    print(f\"w : {model.get_parameter(target = 'linear.weight').item():.3f}\")\n",
        "\n",
        "# try prediction \n",
        "# IMPORTANT TO DETACH \n",
        "prediction = model(x = torch.tensor([[5],[6]], dtype = torch.float32)).detach()\n",
        "print(f\"Predictions :{prediction}\")\n",
        "\n",
        "# plotting\n",
        "y_pred_train = model(x = x).detach().numpy()\n",
        "plt.plot(x_numpy_std, y_numpy_std, 'ro')\n",
        "#plt.plot(x_numpy, y_numpy, 'yo')\n",
        "plt.plot(x_numpy_std, y_pred_train, 'b')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TORCH METHOD 6  : FFN MULTICLASS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:03<00:00, 2618149.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 262009.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 1871203.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# hyperparameters \n",
        "input_size = 784 # 28x28 images\n",
        "hidden_size = 100\n",
        "num_clases = 10\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "lr = 0.001\n",
        "\n",
        "# MNIST\n",
        "train_set = torchvision.datasets.MNIST(root = './data', train = True, transform =transforms.ToTensor() , download = True)\n",
        "test_set = torchvision.datasets.MNIST(root = './data', train = False, transform =transforms.ToTensor() , download = True)\n",
        "\n",
        "# MNIST dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_set, batch_size = batch_size, shuffle = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        }
      ],
      "source": [
        "print(train_set)\n",
        "print(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "nLuoTVEkKzT3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda available:  False\n",
            "\n",
            "\n",
            "\n",
            "Module : NeuralNet(\n",
            "  (l_1): Linear(in_features=784, out_features=100, bias=True)\n",
            "  (Relu): ReLU()\n",
            "  (l_2): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Module : Linear(in_features=784, out_features=100, bias=True)\n",
            "\n",
            "Module : ReLU()\n",
            "\n",
            "Module : Linear(in_features=100, out_features=10, bias=True)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# data samples shape\n",
        "\n",
        "# custom class model (linear regression layer)\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size,hidden_size, num_classes) -> None:\n",
        "       super().__init__()\n",
        "       self.in_size = input_size\n",
        "       self.hidden = hidden_size\n",
        "       self.num_classes = num_classes\n",
        "       \n",
        "       # Arquitecture of FFN\n",
        "       self.l_1 = nn.Linear(in_features= self.in_size, out_features = self.hidden, bias = True)\n",
        "       self.Relu = nn.ReLU()\n",
        "       self.l_2 = nn.Linear(in_features=  self.hidden, out_features = self.num_classes, bias = True)\n",
        "       \n",
        "    def forward(self,x):\n",
        "        return self.l_2(self.Relu(self.l_1(x)))\n",
        "    \n",
        "\n",
        "# instanciar capa y crear modelo\n",
        "model = NeuralNet(input_size =input_size ,hidden_size = hidden_size, num_classes = num_clases)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Cuda available: \",torch.cuda.is_available())\n",
        "model.to(device)\n",
        "# loss class\n",
        "loss = nn.CrossEntropyLoss()\n",
        "# crea optimizer para prescendir de actualizacion de pesos a mano\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
        "\n",
        "# MODULE CLASS DOCU:\n",
        "\"\"\"nn.Module :\n",
        "Base class for all neural network modules.\n",
        "\n",
        "Your models should also subclass this class.\n",
        "\n",
        "Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes: \n",
        "self.sub_module = nn.Linear(...)\"\"\"\n",
        "\n",
        "# Iterate along all de modules inside a network class or model class. Notice that LinearRegression module has inside a linear layer \"module\" or only linear layer\n",
        "# note that the atribute name : self.linear will define the string \"linear\" to refer to that layer inside a module\n",
        "# this will be useful when ,inside a class module, there are several layers\n",
        "print(\"\\n\")\n",
        "\n",
        "for m in model.modules():\n",
        "    print(f\"\\nModule : {m}\")\n",
        "\n",
        "# iterate along all the parameters inside a module ( a module can have a lot of layers with their parameters)\n",
        "for p in model.named_parameters(prefix='', recurse=True, remove_duplicate=True):\n",
        "    #print(f\"\\nParameter : {p}\")\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2neB6EJOLhpB",
        "outputId": "46ce8dd7-b529-4b84-f56c-6d5228a437f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [00:13<02:00, 13.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 0.270\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 2/10 [00:38<02:42, 20.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 1\n",
            "loss : 0.140\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 3/10 [00:52<02:01, 17.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 2\n",
            "loss : 0.167\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [01:06<01:35, 15.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 3\n",
            "loss : 0.107\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [01:19<01:15, 15.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 4\n",
            "loss : 0.029\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 6/10 [01:33<00:58, 14.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 5\n",
            "loss : 0.103\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7/10 [01:46<00:42, 14.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 6\n",
            "loss : 0.032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 8/10 [02:00<00:28, 14.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 7\n",
            "loss : 0.031\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 9/10 [02:14<00:14, 14.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 8\n",
            "loss : 0.028\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [02:27<00:00, 14.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 9\n",
            "loss : 0.032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for epoch in tqdm.tqdm(range(epochs)):\n",
        "  for i ,(b_samples ,b_labels) in enumerate(train_loader):\n",
        "    # (100,1,28,28) -> (100, 28*28)\n",
        "    b_samples = b_samples.view(100,28*28).to(device)\n",
        "    #print(b_samples.shape)\n",
        "    # (100) -> (100,1)\n",
        "    b_labels = b_labels.to(device)\n",
        "    \n",
        "    # forward now is == to calling the model\n",
        "    \n",
        "    y_pred = model(b_samples)\n",
        "    #print(y_pred.shape) # (100,10)\n",
        "\n",
        "    # compute loss for all the batch\n",
        "    l   = loss(input = y_pred, target = b_labels) # b_labels debe ser (100) pq crossentropyloss en torch solo necesita el indice de la label no el vector de dimendion 10 entero tipo one hot\n",
        "\n",
        "    # compute the gradients for each w_i : dloss/dw_i (usa grafo computacional y almacena ,en el tensor .grad asociado a cada parametro, el valor del gradiente que luego usa el algoritmo de optimizacion)\n",
        "    l.backward()\n",
        "\n",
        "    # paso del optimizer (en este caso un simple SGD) equivale a lo que haciamos a mano de w += -lr*w.grad\n",
        "    optimizer.step()\n",
        "    \n",
        "    # restore or eliminate the grads inside parameter.grad tensor for next iteration\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  print(f\"epoch : {epoch}\")\n",
        "  print(f\"loss : {l.item():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuarcy 1 for iter/batch 0 : 99\n",
            "Accuarcy 2 for iter/batch 0 : 99\n",
            "Accuarcy 1 for iter/batch 1 : 97\n",
            "Accuarcy 2 for iter/batch 1 : 97\n",
            "Accuarcy 1 for iter/batch 2 : 97\n",
            "Accuarcy 2 for iter/batch 2 : 97\n",
            "Accuarcy 1 for iter/batch 3 : 97\n",
            "Accuarcy 2 for iter/batch 3 : 97\n",
            "Accuarcy 1 for iter/batch 4 : 97\n",
            "Accuarcy 2 for iter/batch 4 : 97\n",
            "Accuarcy 1 for iter/batch 5 : 99\n",
            "Accuarcy 2 for iter/batch 5 : 99\n",
            "Accuarcy 1 for iter/batch 6 : 96\n",
            "Accuarcy 2 for iter/batch 6 : 96\n",
            "Accuarcy 1 for iter/batch 7 : 96\n",
            "Accuarcy 2 for iter/batch 7 : 96\n",
            "Accuarcy 1 for iter/batch 8 : 98\n",
            "Accuarcy 2 for iter/batch 8 : 98\n",
            "Accuarcy 1 for iter/batch 9 : 96\n",
            "Accuarcy 2 for iter/batch 9 : 96\n",
            "Accuarcy 1 for iter/batch 10 : 96\n",
            "Accuarcy 2 for iter/batch 10 : 96\n",
            "Accuarcy 1 for iter/batch 11 : 94\n",
            "Accuarcy 2 for iter/batch 11 : 94\n",
            "Accuarcy 1 for iter/batch 12 : 93\n",
            "Accuarcy 2 for iter/batch 12 : 93\n",
            "Accuarcy 1 for iter/batch 13 : 97\n",
            "Accuarcy 2 for iter/batch 13 : 97\n",
            "Accuarcy 1 for iter/batch 14 : 100\n",
            "Accuarcy 2 for iter/batch 14 : 100\n",
            "Accuarcy 1 for iter/batch 15 : 95\n",
            "Accuarcy 2 for iter/batch 15 : 95\n",
            "Accuarcy 1 for iter/batch 16 : 97\n",
            "Accuarcy 2 for iter/batch 16 : 97\n",
            "Accuarcy 1 for iter/batch 17 : 96\n",
            "Accuarcy 2 for iter/batch 17 : 96\n",
            "Accuarcy 1 for iter/batch 18 : 97\n",
            "Accuarcy 2 for iter/batch 18 : 97\n",
            "Accuarcy 1 for iter/batch 19 : 96\n",
            "Accuarcy 2 for iter/batch 19 : 96\n",
            "Accuarcy 1 for iter/batch 20 : 92\n",
            "Accuarcy 2 for iter/batch 20 : 92\n",
            "Accuarcy 1 for iter/batch 21 : 94\n",
            "Accuarcy 2 for iter/batch 21 : 94\n",
            "Accuarcy 1 for iter/batch 22 : 96\n",
            "Accuarcy 2 for iter/batch 22 : 96\n",
            "Accuarcy 1 for iter/batch 23 : 97\n",
            "Accuarcy 2 for iter/batch 23 : 97\n",
            "Accuarcy 1 for iter/batch 24 : 96\n",
            "Accuarcy 2 for iter/batch 24 : 96\n",
            "Accuarcy 1 for iter/batch 25 : 99\n",
            "Accuarcy 2 for iter/batch 25 : 99\n",
            "Accuarcy 1 for iter/batch 26 : 96\n",
            "Accuarcy 2 for iter/batch 26 : 96\n",
            "Accuarcy 1 for iter/batch 27 : 100\n",
            "Accuarcy 2 for iter/batch 27 : 100\n",
            "Accuarcy 1 for iter/batch 28 : 97\n",
            "Accuarcy 2 for iter/batch 28 : 97\n",
            "Accuarcy 1 for iter/batch 29 : 95\n",
            "Accuarcy 2 for iter/batch 29 : 95\n",
            "Accuarcy 1 for iter/batch 30 : 99\n",
            "Accuarcy 2 for iter/batch 30 : 99\n",
            "Accuarcy 1 for iter/batch 31 : 98\n",
            "Accuarcy 2 for iter/batch 31 : 98\n",
            "Accuarcy 1 for iter/batch 32 : 99\n",
            "Accuarcy 2 for iter/batch 32 : 99\n",
            "Accuarcy 1 for iter/batch 33 : 100\n",
            "Accuarcy 2 for iter/batch 33 : 100\n",
            "Accuarcy 1 for iter/batch 34 : 97\n",
            "Accuarcy 2 for iter/batch 34 : 97\n",
            "Accuarcy 1 for iter/batch 35 : 92\n",
            "Accuarcy 2 for iter/batch 35 : 92\n",
            "Accuarcy 1 for iter/batch 36 : 96\n",
            "Accuarcy 2 for iter/batch 36 : 96\n",
            "Accuarcy 1 for iter/batch 37 : 93\n",
            "Accuarcy 2 for iter/batch 37 : 93\n",
            "Accuarcy 1 for iter/batch 38 : 96\n",
            "Accuarcy 2 for iter/batch 38 : 96\n",
            "Accuarcy 1 for iter/batch 39 : 96\n",
            "Accuarcy 2 for iter/batch 39 : 96\n",
            "Accuarcy 1 for iter/batch 40 : 96\n",
            "Accuarcy 2 for iter/batch 40 : 96\n",
            "Accuarcy 1 for iter/batch 41 : 98\n",
            "Accuarcy 2 for iter/batch 41 : 98\n",
            "Accuarcy 1 for iter/batch 42 : 95\n",
            "Accuarcy 2 for iter/batch 42 : 95\n",
            "Accuarcy 1 for iter/batch 43 : 95\n",
            "Accuarcy 2 for iter/batch 43 : 95\n",
            "Accuarcy 1 for iter/batch 44 : 96\n",
            "Accuarcy 2 for iter/batch 44 : 96\n",
            "Accuarcy 1 for iter/batch 45 : 98\n",
            "Accuarcy 2 for iter/batch 45 : 98\n",
            "Accuarcy 1 for iter/batch 46 : 99\n",
            "Accuarcy 2 for iter/batch 46 : 99\n",
            "Accuarcy 1 for iter/batch 47 : 100\n",
            "Accuarcy 2 for iter/batch 47 : 100\n",
            "Accuarcy 1 for iter/batch 48 : 95\n",
            "Accuarcy 2 for iter/batch 48 : 95\n",
            "Accuarcy 1 for iter/batch 49 : 97\n",
            "Accuarcy 2 for iter/batch 49 : 97\n",
            "Accuarcy 1 for iter/batch 50 : 99\n",
            "Accuarcy 2 for iter/batch 50 : 99\n",
            "Accuarcy 1 for iter/batch 51 : 98\n",
            "Accuarcy 2 for iter/batch 51 : 98\n",
            "Accuarcy 1 for iter/batch 52 : 100\n",
            "Accuarcy 2 for iter/batch 52 : 100\n",
            "Accuarcy 1 for iter/batch 53 : 99\n",
            "Accuarcy 2 for iter/batch 53 : 99\n",
            "Accuarcy 1 for iter/batch 54 : 98\n",
            "Accuarcy 2 for iter/batch 54 : 98\n",
            "Accuarcy 1 for iter/batch 55 : 100\n",
            "Accuarcy 2 for iter/batch 55 : 100\n",
            "Accuarcy 1 for iter/batch 56 : 97\n",
            "Accuarcy 2 for iter/batch 56 : 97\n",
            "Accuarcy 1 for iter/batch 57 : 98\n",
            "Accuarcy 2 for iter/batch 57 : 98\n",
            "Accuarcy 1 for iter/batch 58 : 97\n",
            "Accuarcy 2 for iter/batch 58 : 97\n",
            "Accuarcy 1 for iter/batch 59 : 92\n",
            "Accuarcy 2 for iter/batch 59 : 92\n",
            "Accuarcy 1 for iter/batch 60 : 98\n",
            "Accuarcy 2 for iter/batch 60 : 98\n",
            "Accuarcy 1 for iter/batch 61 : 99\n",
            "Accuarcy 2 for iter/batch 61 : 99\n",
            "Accuarcy 1 for iter/batch 62 : 100\n",
            "Accuarcy 2 for iter/batch 62 : 100\n",
            "Accuarcy 1 for iter/batch 63 : 99\n",
            "Accuarcy 2 for iter/batch 63 : 99\n",
            "Accuarcy 1 for iter/batch 64 : 98\n",
            "Accuarcy 2 for iter/batch 64 : 98\n",
            "Accuarcy 1 for iter/batch 65 : 93\n",
            "Accuarcy 2 for iter/batch 65 : 93\n",
            "Accuarcy 1 for iter/batch 66 : 99\n",
            "Accuarcy 2 for iter/batch 66 : 99\n",
            "Accuarcy 1 for iter/batch 67 : 96\n",
            "Accuarcy 2 for iter/batch 67 : 96\n",
            "Accuarcy 1 for iter/batch 68 : 100\n",
            "Accuarcy 2 for iter/batch 68 : 100\n",
            "Accuarcy 1 for iter/batch 69 : 100\n",
            "Accuarcy 2 for iter/batch 69 : 100\n",
            "Accuarcy 1 for iter/batch 70 : 99\n",
            "Accuarcy 2 for iter/batch 70 : 99\n",
            "Accuarcy 1 for iter/batch 71 : 100\n",
            "Accuarcy 2 for iter/batch 71 : 100\n",
            "Accuarcy 1 for iter/batch 72 : 99\n",
            "Accuarcy 2 for iter/batch 72 : 99\n",
            "Accuarcy 1 for iter/batch 73 : 100\n",
            "Accuarcy 2 for iter/batch 73 : 100\n",
            "Accuarcy 1 for iter/batch 74 : 99\n",
            "Accuarcy 2 for iter/batch 74 : 99\n",
            "Accuarcy 1 for iter/batch 75 : 99\n",
            "Accuarcy 2 for iter/batch 75 : 99\n",
            "Accuarcy 1 for iter/batch 76 : 100\n",
            "Accuarcy 2 for iter/batch 76 : 100\n",
            "Accuarcy 1 for iter/batch 77 : 100\n",
            "Accuarcy 2 for iter/batch 77 : 100\n",
            "Accuarcy 1 for iter/batch 78 : 98\n",
            "Accuarcy 2 for iter/batch 78 : 98\n",
            "Accuarcy 1 for iter/batch 79 : 99\n",
            "Accuarcy 2 for iter/batch 79 : 99\n",
            "Accuarcy 1 for iter/batch 80 : 98\n",
            "Accuarcy 2 for iter/batch 80 : 98\n",
            "Accuarcy 1 for iter/batch 81 : 98\n",
            "Accuarcy 2 for iter/batch 81 : 98\n",
            "Accuarcy 1 for iter/batch 82 : 100\n",
            "Accuarcy 2 for iter/batch 82 : 100\n",
            "Accuarcy 1 for iter/batch 83 : 99\n",
            "Accuarcy 2 for iter/batch 83 : 99\n",
            "Accuarcy 1 for iter/batch 84 : 98\n",
            "Accuarcy 2 for iter/batch 84 : 98\n",
            "Accuarcy 1 for iter/batch 85 : 97\n",
            "Accuarcy 2 for iter/batch 85 : 97\n",
            "Accuarcy 1 for iter/batch 86 : 100\n",
            "Accuarcy 2 for iter/batch 86 : 100\n",
            "Accuarcy 1 for iter/batch 87 : 100\n",
            "Accuarcy 2 for iter/batch 87 : 100\n",
            "Accuarcy 1 for iter/batch 88 : 100\n",
            "Accuarcy 2 for iter/batch 88 : 100\n",
            "Accuarcy 1 for iter/batch 89 : 100\n",
            "Accuarcy 2 for iter/batch 89 : 100\n",
            "Accuarcy 1 for iter/batch 90 : 96\n",
            "Accuarcy 2 for iter/batch 90 : 96\n",
            "Accuarcy 1 for iter/batch 91 : 100\n",
            "Accuarcy 2 for iter/batch 91 : 100\n",
            "Accuarcy 1 for iter/batch 92 : 99\n",
            "Accuarcy 2 for iter/batch 92 : 99\n",
            "Accuarcy 1 for iter/batch 93 : 100\n",
            "Accuarcy 2 for iter/batch 93 : 100\n",
            "Accuarcy 1 for iter/batch 94 : 97\n",
            "Accuarcy 2 for iter/batch 94 : 97\n",
            "Accuarcy 1 for iter/batch 95 : 98\n",
            "Accuarcy 2 for iter/batch 95 : 98\n",
            "Accuarcy 1 for iter/batch 96 : 96\n",
            "Accuarcy 2 for iter/batch 96 : 96\n",
            "Accuarcy 1 for iter/batch 97 : 92\n",
            "Accuarcy 2 for iter/batch 97 : 92\n",
            "Accuarcy 1 for iter/batch 98 : 98\n",
            "Accuarcy 2 for iter/batch 98 : 98\n",
            "Accuarcy 1 for iter/batch 99 : 98\n",
            "Accuarcy 2 for iter/batch 99 : 98\n",
            "Total accuracy 1 : 0.9746999740600586\n",
            "Total accuracy 2: 0.9746999740600586\n"
          ]
        }
      ],
      "source": [
        "# Evaluation / metrics \n",
        "with torch.no_grad():\n",
        "    accuracy = 0\n",
        "    accuracy_2 = 0  \n",
        "    samples = 0\n",
        "    for i, (b_images,b_labels) in enumerate(test_loader):\n",
        "        samples += 100\n",
        "        b_images = b_images.view(100, 28*28)\n",
        "        y_test_pred = model(b_images)\n",
        "        \n",
        "        # Segundo metodo accuracy\n",
        "        _,pred_class_2 = torch.max(y_test_pred ,dim =  1)\n",
        "        accuracy_2 += (pred_class_2 == b_labels).sum()\n",
        "        \n",
        "        # Primer metodo accurcy\n",
        "        pred_class = y_test_pred.argmax(dim = 1)\n",
        "        accuracy += torch.sum(pred_class == b_labels)\n",
        "        print(f\"Accuarcy 1 for iter/batch {i} : {torch.sum(pred_class == b_labels)}\")\n",
        "        print(f\"Accuarcy 2 for iter/batch {i} : {(pred_class_2 == b_labels).sum()}\")\n",
        "\n",
        "\n",
        "    accuracy = accuracy / samples\n",
        "    accuracy_2 = accuracy_2 / samples \n",
        "    print(\"Total accuracy 1 :\", accuracy.item())\n",
        "    print(\"Total accuracy 2:\", accuracy_2.item())\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TORCH : LOGISTIC REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyNsbbhcOxR5",
        "outputId": "d344168f-8c5b-45c1-b866-8a359f4ad548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(398, 30) (398,) (171, 30) (171,)\n",
            "torch.Size([398, 30]) torch.Size([398]) torch.Size([171, 30]) torch.Size([171])\n",
            "Comprobacion de si Y_train codificado probabilidades o one hot (0/1) tensor([1., 1., 0., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# hyperparameters \n",
        "batch_size = 20\n",
        "num_clases = 2\n",
        "num_epochs = 100\n",
        "lr = 0.01\n",
        "\n",
        "# DATA\n",
        "bc = datasets.load_breast_cancer()\n",
        "x,y = bc.data, bc.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, train_size = 0.7, test_size = 0.3, random_state = 1234)\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "\n",
        "# std\n",
        "sc = StandardScaler()\n",
        "sc.fit(x_train)\n",
        "x_train_std = sc.transform(x_train)\n",
        "x_test_std = sc.transform(x_test)\n",
        "\n",
        "# numpy -> tensor\n",
        "X_train_std = torch.from_numpy(x_train_std.astype(np.float32))\n",
        "Y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "X_test_std = torch.from_numpy(x_test_std.astype(np.float32))\n",
        "Y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "print(X_train_std.shape, Y_train.shape, X_test_std.shape, Y_test.shape)\n",
        "print(f\"Comprobacion de si Y_train codificado probabilidades o one hot (0/1) {Y_train[0:5]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VCjpi0oiL9g",
        "outputId": "82ec7283-dfb7-4fdc-810c-d2c10d398540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Module : LogisticRegression(\n",
            "  (l_1): Linear(in_features=30, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "\n",
            "Module : Linear(in_features=30, out_features=1, bias=True)\n",
            "\n",
            "Module : Sigmoid()\n",
            "\n",
            "Parameter : ('l_1.weight', Parameter containing:\n",
            "tensor([[ 0.0449,  0.1248,  0.0375,  0.0530,  0.0539, -0.0189,  0.1298,  0.0623,\n",
            "          0.1515,  0.1021,  0.1508,  0.0664, -0.1004, -0.1302,  0.1029,  0.0196,\n",
            "         -0.0349, -0.1355, -0.1611, -0.0982, -0.0774,  0.1115,  0.0163, -0.1495,\n",
            "         -0.0345,  0.1571,  0.0676, -0.0299,  0.0386, -0.0668]],\n",
            "       requires_grad=True))\n",
            "\n",
            "Parameter : ('l_1.bias', Parameter containing:\n",
            "tensor([-0.0826], requires_grad=True))\n"
          ]
        }
      ],
      "source": [
        "# model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self,features) -> None:\n",
        "        super().__init__()\n",
        "        self.l_1 = nn.Linear(in_features = features, out_features = 1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self,x):\n",
        "        return self.sigmoid(self.l_1(x))\n",
        "\n",
        "# model object\n",
        "model = LogisticRegression(features = X_train_std.shape[1])\n",
        "\n",
        "# loss function\n",
        "loss = nn.BCELoss()\n",
        "\n",
        "# optimizer \n",
        "adam = torch.optim.Adam(params = model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08)\n",
        "\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "for m in model.modules():\n",
        "    print(f\"\\nModule : {m}\")\n",
        "\n",
        "# iterate along all the parameters inside a module ( a module can have a lot of layers with their parameters)\n",
        "for p in model.named_parameters(prefix='', recurse=True, remove_duplicate=True):\n",
        "    print(f\"\\nParameter : {p}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUo6snO-ivUr",
        "outputId": "583d06ad-cb10-4341-bc94-dc84755ceb05"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 6/100 [00:00<00:02, 46.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 0.204\n",
            "epoch : 1\n",
            "loss : 0.107\n",
            "epoch : 2\n",
            "loss : 0.078\n",
            "epoch : 3\n",
            "loss : 0.062\n",
            "epoch : 4\n",
            "loss : 0.052\n",
            "epoch : 5\n",
            "loss : 0.045\n",
            "epoch : 6\n",
            "loss : 0.039\n",
            "epoch : 7\n",
            "loss : 0.035\n",
            "epoch : 8\n",
            "loss : 0.032\n",
            "epoch : 9\n",
            "loss : 0.029\n",
            "epoch : 10\n",
            "loss : 0.027\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 25/100 [00:00<00:00, 79.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 11\n",
            "loss : 0.025\n",
            "epoch : 12\n",
            "loss : 0.023\n",
            "epoch : 13\n",
            "loss : 0.022\n",
            "epoch : 14\n",
            "loss : 0.020\n",
            "epoch : 15\n",
            "loss : 0.019\n",
            "epoch : 16\n",
            "loss : 0.018\n",
            "epoch : 17\n",
            "loss : 0.017\n",
            "epoch : 18\n",
            "loss : 0.016\n",
            "epoch : 19\n",
            "loss : 0.015\n",
            "epoch : 20\n",
            "loss : 0.015\n",
            "epoch : 21\n",
            "loss : 0.014\n",
            "epoch : 22\n",
            "loss : 0.013\n",
            "epoch : 23\n",
            "loss : 0.013\n",
            "epoch : 24\n",
            "loss : 0.012\n",
            "epoch : 25\n",
            "loss : 0.012\n",
            "epoch : 26\n",
            "loss : 0.011\n",
            "epoch : 27\n",
            "loss : 0.011\n",
            "epoch : 28\n",
            "loss : 0.010\n",
            "epoch : 29\n",
            "loss : 0.010\n",
            "epoch : 30\n",
            "loss : 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 44/100 [00:00<00:00, 85.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 31\n",
            "loss : 0.009\n",
            "epoch : 32\n",
            "loss : 0.009\n",
            "epoch : 33\n",
            "loss : 0.009\n",
            "epoch : 34\n",
            "loss : 0.009\n",
            "epoch : 35\n",
            "loss : 0.008\n",
            "epoch : 36\n",
            "loss : 0.008\n",
            "epoch : 37\n",
            "loss : 0.008\n",
            "epoch : 38\n",
            "loss : 0.008\n",
            "epoch : 39\n",
            "loss : 0.007\n",
            "epoch : 40\n",
            "loss : 0.007\n",
            "epoch : 41\n",
            "loss : 0.007\n",
            "epoch : 42\n",
            "loss : 0.007\n",
            "epoch : 43\n",
            "loss : 0.007\n",
            "epoch : 44\n",
            "loss : 0.006\n",
            "epoch : 45\n",
            "loss : 0.006\n",
            "epoch : 46\n",
            "loss : 0.006\n",
            "epoch : 47\n",
            "loss : 0.006\n",
            "epoch : 48\n",
            "loss : 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 62/100 [00:00<00:00, 87.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 49\n",
            "loss : 0.006\n",
            "epoch : 50\n",
            "loss : 0.006\n",
            "epoch : 51\n",
            "loss : 0.005\n",
            "epoch : 52\n",
            "loss : 0.005\n",
            "epoch : 53\n",
            "loss : 0.005\n",
            "epoch : 54\n",
            "loss : 0.005\n",
            "epoch : 55\n",
            "loss : 0.005\n",
            "epoch : 56\n",
            "loss : 0.005\n",
            "epoch : 57\n",
            "loss : 0.005\n",
            "epoch : 58\n",
            "loss : 0.005\n",
            "epoch : 59\n",
            "loss : 0.005\n",
            "epoch : 60\n",
            "loss : 0.004\n",
            "epoch : 61\n",
            "loss : 0.004\n",
            "epoch : 62\n",
            "loss : 0.004\n",
            "epoch : 63\n",
            "loss : 0.004\n",
            "epoch : 64\n",
            "loss : 0.004\n",
            "epoch : 65\n",
            "loss : 0.004\n",
            "epoch : 66\n",
            "loss : 0.004\n",
            "epoch : 67\n",
            "loss : 0.004\n",
            "epoch : 68\n",
            "loss : 0.004\n",
            "epoch : 69\n",
            "loss : 0.004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 83/100 [00:00<00:00, 89.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 70\n",
            "loss : 0.004\n",
            "epoch : 71\n",
            "loss : 0.004\n",
            "epoch : 72\n",
            "loss : 0.004\n",
            "epoch : 73\n",
            "loss : 0.004\n",
            "epoch : 74\n",
            "loss : 0.004\n",
            "epoch : 75\n",
            "loss : 0.003\n",
            "epoch : 76\n",
            "loss : 0.003\n",
            "epoch : 77\n",
            "loss : 0.003\n",
            "epoch : 78\n",
            "loss : 0.003\n",
            "epoch : 79\n",
            "loss : 0.003\n",
            "epoch : 80\n",
            "loss : 0.003\n",
            "epoch : 81\n",
            "loss : 0.003\n",
            "epoch : 82\n",
            "loss : 0.003\n",
            "epoch : 83\n",
            "loss : 0.003\n",
            "epoch : 84\n",
            "loss : 0.003\n",
            "epoch : 85\n",
            "loss : 0.003\n",
            "epoch : 86\n",
            "loss : 0.003\n",
            "epoch : 87\n",
            "loss : 0.003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 80.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 88\n",
            "loss : 0.003\n",
            "epoch : 89\n",
            "loss : 0.003\n",
            "epoch : 90\n",
            "loss : 0.003\n",
            "epoch : 91\n",
            "loss : 0.003\n",
            "epoch : 92\n",
            "loss : 0.003\n",
            "epoch : 93\n",
            "loss : 0.003\n",
            "epoch : 94\n",
            "loss : 0.003\n",
            "epoch : 95\n",
            "loss : 0.003\n",
            "epoch : 96\n",
            "loss : 0.003\n",
            "epoch : 97\n",
            "loss : 0.003\n",
            "epoch : 98\n",
            "loss : 0.003\n",
            "epoch : 99\n",
            "loss : 0.003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "  for i in range(0,X_train_std.shape[0],batch_size):\n",
        "    if batch_size+i >  X_train_std.shape[0]:\n",
        "      last_index = X_train_std.shape[0]\n",
        "      b_samples =  X_train_std.shape[0]  - i \n",
        "    else:\n",
        "      last_index = batch_size + i\n",
        "      b_samples = batch_size\n",
        "      \n",
        "    b_x = X_train_std[i:last_index,:]\n",
        "    b_y = Y_train[i:last_index]\n",
        "\n",
        "    # forward\n",
        "    y_pred = model(b_x).view(b_samples)\n",
        "    \n",
        "\n",
        "    # loss == computar grafo computacional de la funcion perdida hacia atras para despues calcular gradientes de forma efectiva\n",
        "    l = loss(input = y_pred, target = b_y)\n",
        "    \n",
        "    # backwardpropa == calculo de gradientes de loss respecto pesos y alamacena en tensor .grad asociado a tensor de parametros\n",
        "    l.backward()\n",
        "    \n",
        "    # paso del optimizer == recoge gradientes calculados (dentro de tensor parameters.grad ) y actualiza pesos en funcion algoritmo de optimizacion elegido\n",
        "    adam.step()\n",
        "    \n",
        "    # borrado de gradientes dentro del tensor\n",
        "    adam.zero_grad()\n",
        "    \n",
        "  print(f\"epoch : {epoch}\")\n",
        "  print(f\"loss : {l.item():.3f}\")\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([171, 1])\n",
            "Total accuracy 1 : 0.9473684430122375\n",
            "Total accuracy 2 : 0.9473684430122375\n"
          ]
        }
      ],
      "source": [
        "# Evaluation / metrics \n",
        "with torch.no_grad():\n",
        "    y_test_pred_proba = model(X_test_std)\n",
        "    print(y_test_pred.shape)\n",
        "    \n",
        "    # Establece umbral de ser o no de la clase 1 en funcion de probabilidad; recuerda que y_pred son las probabilidades de pertenecer a clase 1\n",
        "    # es decir se transforma el tensor de probabilidad en un tensor de prediccion de clase 0 o 1\n",
        "    threshold = 0.5\n",
        "    \n",
        "    # forma 1\n",
        "    y_test_pred_class = torch.tensor([1 if prob_i > threshold else 0 for prob_i in y_test_pred_proba], dtype = torch.int)\n",
        "    \n",
        "    # forma 2\n",
        "    y_test_pred_class_2 = torch.zeros(y_test_pred_proba.shape[0])\n",
        "    for i,y_i in enumerate(y_test_pred_proba.view(y_test_pred_proba.shape[0])):\n",
        "        if y_i.item() > threshold:\n",
        "            y_test_pred_class_2[i] = 1\n",
        "        else:\n",
        "            y_test_pred_class_2[i] = 0\n",
        "    \n",
        "    # Metodo accuracy\n",
        "    accuracy = torch.sum(y_test_pred_class == Y_test) / X_test_std.shape[0]\n",
        "    accuracy_2 = torch.sum(y_test_pred_class_2 == Y_test) / X_test_std.shape[0]\n",
        "    \n",
        "    \n",
        "    print(\"Total accuracy 1 :\", accuracy.item())\n",
        "    print(\"Total accuracy 2 :\", accuracy_2.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr1JvHD8_VG9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
