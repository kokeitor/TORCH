{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JVQ13l32QpB",
        "outputId": "79961510-ee34-4c42-8373-8bfb55d03ed4"
      },
      "outputs": [],
      "source": [
        "# Esto es solo para poder debugear.\n",
        "#!pip install torch tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "# Genera una semilla fija para que los experimentos sea repetibles.\n",
        "t_cg = torch.manual_seed(1547)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MbJDaOcZGIr"
      },
      "source": [
        "### NUMPY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k40zE64ZUegJ"
      },
      "outputs": [],
      "source": [
        "x = np.array([[1, 2,3, 4]],dtype = np.float32)\n",
        "y = np.array([2, 4,6,8], dtype = np.float32)\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n",
        "def loss(y_pred, y):\n",
        "  return np.mean(((y_pred - y)**2))\n",
        "\n",
        "def grad(y_pred,y):\n",
        "  return (x* 2 * ((y_pred) - y)).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5ayDy4JU3Q9",
        "outputId": "a608d9bd-16f8-47aa-af7d-dd44502c2683"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30.0 1.5\n",
            "1.875 1.875\n",
            "0.1171875 1.96875\n",
            "0.0073242188 1.9921875\n",
            "0.00045776367 1.998046875\n",
            "2.861023e-05 1.99951171875\n",
            "1.7881393e-06 1.9998779296875\n",
            "1.1175871e-07 1.999969482421875\n",
            "6.9849193e-09 1.9999923706054688\n",
            "4.3655746e-10 1.9999980926513672\n",
            "9.999990463256836\n"
          ]
        }
      ],
      "source": [
        "w= 0.0\n",
        "lr =0.05\n",
        "for e in range(10):\n",
        "  y_pred = forward(x)\n",
        "  loss_i = loss(y_pred,y)\n",
        "  grad_i = grad(y_pred,y)\n",
        "  w -= lr*grad_i\n",
        "  print(loss_i, w)\n",
        "\n",
        "print(forward(x = 5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1RouJqWZHQ3"
      },
      "source": [
        "### TORCH METHOD 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZEedSeCoZFT4"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1,2,3, 4],dtype = torch.float32, requires_grad = False)\n",
        "y = torch.tensor([2,4,6,8], dtype = torch.float32, requires_grad = False)\n",
        "w = torch.tensor(0.0,dtype = torch.float32, requires_grad = True)\n",
        "\n",
        "\n",
        "def forward(x):\n",
        "  return w *  x\n",
        "def loss(y_pred, y):\n",
        "  return ((y_pred - y)**2).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPV0BhWwhbaS",
        "outputId": "9f3b845e-e48e-4a47-ee54-adad8f82c58e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 1786.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 30.000\n",
            "w : 0.300\n",
            "epoch : 10\n",
            "loss : 1.163\n",
            "w : 1.665\n",
            "epoch : 20\n",
            "loss : 0.045\n",
            "w : 1.934\n",
            "epoch : 30\n",
            "loss : 0.002\n",
            "w : 1.987\n",
            "epoch : 40\n",
            "loss : 0.000\n",
            "w : 1.997\n",
            "9.997042655944824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "lr =0.01\n",
        "for e in tqdm.tqdm(range(50)):\n",
        "  y_pred = forward(x)\n",
        "  l  = loss(y_pred, y)\n",
        "  l.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    w -= lr * w.grad\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if e % 10 == 0:\n",
        "    print(f\"epoch : {e}\")\n",
        "    print(f\"loss : {l.item():.3f}\")\n",
        "    print(f\"w : {w.item():.3f}\")\n",
        "\n",
        "print(forward(x = 5).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBUak5v2gJYp"
      },
      "source": [
        "### TORCH METHOD 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4OUYDA6mJfMm"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "x = torch.tensor([1,2,3, 4],dtype = torch.float32, requires_grad = False)\n",
        "y = torch.tensor([2,4,6,8], dtype = torch.float32, requires_grad = False)\n",
        "w = torch.tensor(0.0,dtype = torch.float32, requires_grad = True)\n",
        "\n",
        "#otra forma seria : instanciar clase que devuelve objeto MSEloss\n",
        "# loss = nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt8l9pjvPuH4",
        "outputId": "34afe7b2-0c27-406c-dca6-831d92e4aa0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 3845.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 30.000\n",
            "w : 0.300\n",
            "epoch : 10\n",
            "loss : 1.163\n",
            "w : 1.665\n",
            "epoch : 20\n",
            "loss : 0.045\n",
            "w : 1.934\n",
            "epoch : 30\n",
            "loss : 0.002\n",
            "w : 1.987\n",
            "epoch : 40\n",
            "loss : 0.000\n",
            "w : 1.997\n",
            "9.997042655944824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "lr =0.01\n",
        "for e in tqdm.tqdm(range(50)):\n",
        "  y_pred = forward(x)\n",
        "  # simula la funcion de loss implementada a mano antes\n",
        "\n",
        "  # Metodo 1: instanciando clase de error\n",
        "  # l = loss(input = y_pred, target = y)\n",
        "\n",
        "  # Metodo 2 : simulan la funcion de loss implementada a mano antes mediante una funcion de error built in de torch:\n",
        "  l  = nn.functional.mse_loss(input = y_pred, target = y, size_average=None, reduce=None, reduction='mean')\n",
        "  l.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    w -= lr * w.grad\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if e % 10 == 0:\n",
        "    print(f\"epoch : {e}\")\n",
        "    print(f\"loss : {l.item():.3f}\")\n",
        "    print(f\"w : {w.item():.3f}\")\n",
        "\n",
        "print(forward(x = 5).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEYAJ4W9hr5X"
      },
      "source": [
        "### TORCH METHOD 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3yaq1ZBRQIqq"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "x = torch.tensor([1,2,3, 4],dtype = torch.float32, requires_grad = False)\n",
        "y = torch.tensor([2,4,6,8], dtype = torch.float32, requires_grad = False)\n",
        "w = torch.tensor(0.0,dtype = torch.float32, requires_grad = True)\n",
        "\n",
        "# loss class\n",
        "loss = nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
        "\n",
        "# crea optimizer para prescendir d actualizacion de pesos a mano\n",
        "optimizer = torch.optim.SGD(params = [w], lr=0.01)\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZfjHjk2iBTU",
        "outputId": "f645192e-1657-4664-ec98-d50fedc36007"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 543.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 30.000\n",
            "w : 0.300\n",
            "epoch : 10\n",
            "loss : 1.163\n",
            "w : 1.665\n",
            "epoch : 20\n",
            "loss : 0.045\n",
            "w : 1.934\n",
            "epoch : 30\n",
            "loss : 0.002\n",
            "w : 1.987\n",
            "epoch : 40\n",
            "loss : 0.000\n",
            "w : 1.997\n",
            "9.997042655944824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in tqdm.tqdm(range(50)):\n",
        "  # forward\n",
        "  y_pred = forward(x)\n",
        "\n",
        "  # compute loss for all the batch\n",
        "  l   = loss(input = y_pred, target = y)\n",
        "\n",
        "  # compute the gradients for each w_i : dloss/dw_i (usa grafo computacional y almacena ,en el tensor .grad asociado a cada parametro, el valor del gradiente que luego usa el algoritmo de optimizacion)\n",
        "  l.backward()\n",
        "\n",
        "  # paso del optimizer (en este caso un simple SGD) equivale a lo que haciamos a mano de w += -lr*w.grad\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"epoch : {epoch}\")\n",
        "    print(f\"loss : {l.item():.3f}\")\n",
        "    print(f\"w : {w.item():.3f}\")\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "print(forward(x = 5).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eft4lJyOqM9q"
      },
      "source": [
        "### TORCH METHOD 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "BYPG397vqPYO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "x = torch.tensor([[1],[2],[3], [4]],dtype = torch.float32, requires_grad = False)\n",
        "y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32, requires_grad = False)\n",
        "\n",
        "# Dimension layers (in and out, entra 4 samples (batch) sale 1 output por cada input) teniendo en cuenta el batch: X_input(1,4) * W(4,1)--> (1,1) // sin batch : X_input(1) * W(1)--> (1)\n",
        "batch_size , features  = x.shape\n",
        "# features == dim_input y en este caso features ==  dim_output pq capa solo 1 parametro (como que solo una neurona)\n",
        "\n",
        "# model [capa lineal inicializa un tensor paramters asociado a la capa en funcion de las input y output dimesnions especificadas ]: capa lineal con 1 W y 1 X input (escalares) ; luego dim_input = 1 y dim_output = 1\n",
        "model = nn.Linear(in_features= 1, out_features = 1)\n",
        "\n",
        "# loss class\n",
        "loss = nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
        "\n",
        "# crea optimizer para prescendir de actualizacion de pesos a mano\n",
        "optimizer = torch.optim.SGD(params = model.parameters(), lr=0.01)\n",
        "\n",
        "def forward(x):\n",
        "  return x * w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrJjVppMsOQC",
        "outputId": "34a42a02-75f1-4df4-9e3b-794be874aec0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 21.203\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Linear' object has no attribute 'get_parameters'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[65], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parameters\u001b[49m()\u001b[38;5;241m.\u001b[39mitems()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m   optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(forward(x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[1;32mc:\\Users\\Jorge\\Desktop\\MASTER_IA\\TORCH\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'get_parameters'"
          ]
        }
      ],
      "source": [
        "for epoch in tqdm.tqdm(range(50)):\n",
        "  # forward now is == to calling the model\n",
        "  y_pred = model(x)\n",
        "\n",
        "  # compute loss for all the batch\n",
        "  l   = loss(input = y_pred, target = y)\n",
        "\n",
        "  # compute the gradients for each w_i : dloss/dw_i (usa grafo computacional y almacena ,en el tensor .grad asociado a cada parametro, el valor del gradiente que luego usa el algoritmo de optimizacion)\n",
        "  l.backward()\n",
        "\n",
        "  # paso del optimizer (en este caso un simple SGD) equivale a lo que haciamos a mano de w += -lr*w.grad\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"epoch : {epoch}\")\n",
        "    print(f\"loss : {l.item():.3f}\")\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "print(forward(x = 5).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNpS7kFUkohP"
      },
      "source": [
        "### TORCH METHOD 5  : custom module + dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "znZayzMfktvh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 1) (100,)\n",
            "(100, 1) (100,)\n",
            "torch.Size([100, 1]) torch.Size([100])\n",
            "torch.Size([100, 1]) torch.Size([100, 1])\n",
            "Media de target y : -0.000\n",
            "Media de target y std: -0.000\n",
            "\n",
            "\n",
            "\n",
            "Module : LinearRegression(\n",
            "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Module : Linear(in_features=1, out_features=1, bias=True)\n",
            "\n",
            "Parameter : ('linear.weight', Parameter containing:\n",
            "tensor([[0.9165]], requires_grad=True))\n",
            "\n",
            "Parameter : ('linear.bias', Parameter containing:\n",
            "tensor([-0.5463], requires_grad=True))\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# data samples\n",
        "x_numpy , y_numpy = datasets.make_regression(n_samples =100, n_features= 1, noise =20, random_state = 1)\n",
        "x_numpy_std  = (x_numpy - np.mean(x_numpy)) / np.std(x_numpy)\n",
        "y_numpy_std = (y_numpy - np.mean(y_numpy)) / np.std(y_numpy)\n",
        "print(x_numpy.shape, y_numpy.shape)\n",
        "print(x_numpy_std.shape, y_numpy_std.shape)\n",
        "\n",
        "# to torch tensor\n",
        "x = torch.from_numpy(x_numpy_std.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy_std.astype(np.float32))\n",
        "print(x.shape, y.shape)\n",
        "# reshape y from (100) tensor -> (100,1) tensor\n",
        "y = y.view(y.shape[0],1)\n",
        "print(x.shape, y.shape)\n",
        "print(f\"Media de target y : {y.mean(dim = 0).item():.3f}\")\n",
        "print(f\"Media de target y std: {y.mean(dim = 0).item():.3f}\")\n",
        "\n",
        "# numbeer of samples and number of features\n",
        "# x (100,1)\n",
        "n_samples = x.shape[0]\n",
        "features = x.shape[1]\n",
        "\n",
        "# custom class model (linear regression layer)\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, input_size, output_size) -> None:\n",
        "       super().__init__()\n",
        "       self.in_size = input_size\n",
        "       self.out_size = output_size\n",
        "       # model [capa lineal inicializa un tensor paramters asociado a la capa en funcion de las input y output dimesnions especificadas ]: capa lineal con 1 W y 1 X input (escalares) ; luego dim_input = 1 y dim_output = 1\n",
        "       self.linear = nn.Linear(in_features= self.in_size, out_features = self.out_size, bias = True)\n",
        "    def forward(self,x):\n",
        "        return self.linear(x)\n",
        "    \n",
        "\n",
        "# instanciar capa y crear modelo\n",
        "model = LinearRegression(input_size = features, output_size = features)\n",
        "# loss class\n",
        "loss = nn.MSELoss(reduction='mean')\n",
        "# crea optimizer para prescendir de actualizacion de pesos a mano\n",
        "optimizer = torch.optim.SGD(params = model.parameters(), lr=0.01)\n",
        "\n",
        "# MODULE CLASS DOCU:\n",
        "\"\"\"nn.Module :\n",
        "Base class for all neural network modules.\n",
        "\n",
        "Your models should also subclass this class.\n",
        "\n",
        "Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes: \n",
        "self.sub_module = nn.Linear(...)\"\"\"\n",
        "\n",
        "# Iterate along all de modules inside a network class or model class. Notice that LinearRegression module has inside a linear layer \"module\" or only linear layer\n",
        "# note that the atribute name : self.linear will define the string \"linear\" to refer to that layer inside a module\n",
        "# this will be useful when ,inside a class module, there are several layers\n",
        "print(\"\\n\")\n",
        "\n",
        "for m in model.modules():\n",
        "    print(f\"\\nModule : {m}\")\n",
        "\n",
        "# iterate along all the parameters inside a module ( a module can have a lot of layers with their parameters)\n",
        "for p in model.named_parameters(prefix='', recurse=True, remove_duplicate=True):\n",
        "    print(f\"\\nParameter : {p}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APdfIuxGQTuu",
        "outputId": "8d679be4-54fe-499b-86be-548f69d2049f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 1515.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 0.360\n",
            "w : 0.918\n",
            "Predictions :tensor([[4.5543],\n",
            "        [5.5050]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "epochs = 50\n",
        "for epoch in tqdm.tqdm(range(epochs)):\n",
        "    \n",
        "  # forward now is == to calling the model\n",
        "  y_pred = model(x)\n",
        "\n",
        "  # compute loss for all the batch\n",
        "  l   = loss(input = y_pred, target = y)\n",
        "\n",
        "  # compute the gradients for each w_i : dloss/dw_i (usa grafo computacional y almacena ,en el tensor .grad asociado a cada parametro, el valor del gradiente que luego usa el algoritmo de optimizacion)\n",
        "  l.backward()\n",
        "\n",
        "  # paso del optimizer (en este caso un simple SGD) equivale a lo que haciamos a mano de w += -lr*w.grad\n",
        "  optimizer.step()\n",
        "  \n",
        "  # restore or eliminate the grads inside parameter.grad tensor for next iteration\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 50 == 0:\n",
        "    print(f\"epoch : {epoch}\")\n",
        "    print(f\"loss : {l.item():.3f}\")\n",
        "    print(f\"w : {model.get_parameter(target = 'linear.weight').item():.3f}\")\n",
        "\n",
        "# try prediction \n",
        "# IMPORTANT TO DETACH \n",
        "prediction = model(x = torch.tensor([[5],[6]], dtype = torch.float32)).detach()\n",
        "print(f\"Predictions :{prediction}\")\n",
        "\n",
        "# plotting\n",
        "y_pred_train = model(x = x).detach().numpy()\n",
        "plt.plot(x_numpy_std, y_numpy_std, 'ro')\n",
        "#plt.plot(x_numpy, y_numpy, 'yo')\n",
        "plt.plot(x_numpy_std, y_pred_train, 'b')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TORCH METHOD 6  : FFN "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:03<00:00, 2618149.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 262009.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 1871203.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# hyperparameters \n",
        "input_size = 784 # 28x28 images\n",
        "hidden_size = 100\n",
        "num_clases = 10\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "lr = 0.001\n",
        "\n",
        "# MNIST\n",
        "train_set = torchvision.datasets.MNIST(root = './data', train = True, transform =transforms.ToTensor() , download = True)\n",
        "test_set = torchvision.datasets.MNIST(root = './data', train = False, transform =transforms.ToTensor() , download = True)\n",
        "\n",
        "# MNIST dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_set, batch_size = batch_size, shuffle = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "nLuoTVEkKzT3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Module : NeuralNet(\n",
            "  (l_1): Linear(in_features=784, out_features=100, bias=True)\n",
            "  (Relu): ReLU()\n",
            "  (l_2): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Module : Linear(in_features=784, out_features=100, bias=True)\n",
            "\n",
            "Module : ReLU()\n",
            "\n",
            "Module : Linear(in_features=100, out_features=10, bias=True)\n",
            "\n",
            "Parameter : ('l_1.weight', Parameter containing:\n",
            "tensor([[-1.9008e-02, -8.3813e-03, -1.6468e-02,  ...,  2.9265e-02,\n",
            "          2.7495e-02, -3.1858e-02],\n",
            "        [-3.2442e-02,  3.4523e-02, -3.5707e-02,  ...,  3.3565e-02,\n",
            "          2.4717e-02,  1.5751e-02],\n",
            "        [ 9.4932e-03,  3.4715e-02,  8.6638e-03,  ...,  3.9738e-04,\n",
            "          1.2902e-02,  1.3167e-02],\n",
            "        ...,\n",
            "        [ 3.1463e-02, -3.8930e-04,  6.5913e-04,  ...,  8.4984e-03,\n",
            "          2.6184e-02,  1.4366e-02],\n",
            "        [ 1.9162e-02, -1.4891e-02, -2.3189e-02,  ...,  1.1005e-02,\n",
            "         -1.9193e-02, -1.2443e-02],\n",
            "        [-2.7918e-02,  2.5634e-02,  1.9917e-02,  ...,  3.2149e-06,\n",
            "          6.2249e-03, -2.0274e-02]], requires_grad=True))\n",
            "\n",
            "Parameter : ('l_1.bias', Parameter containing:\n",
            "tensor([-0.0330, -0.0157,  0.0045,  0.0230,  0.0178, -0.0159, -0.0135,  0.0148,\n",
            "         0.0189, -0.0346,  0.0054, -0.0015,  0.0102,  0.0028, -0.0335, -0.0154,\n",
            "         0.0102, -0.0014,  0.0278,  0.0317,  0.0231, -0.0083, -0.0028,  0.0130,\n",
            "        -0.0341, -0.0031,  0.0036, -0.0311, -0.0173, -0.0342,  0.0164,  0.0055,\n",
            "         0.0137, -0.0098, -0.0346, -0.0125,  0.0140, -0.0324,  0.0240,  0.0061,\n",
            "         0.0115, -0.0128, -0.0284, -0.0190,  0.0334,  0.0291,  0.0113,  0.0283,\n",
            "         0.0014, -0.0089,  0.0193,  0.0085,  0.0290, -0.0283,  0.0184, -0.0200,\n",
            "        -0.0109,  0.0323,  0.0344, -0.0013,  0.0204, -0.0099,  0.0068,  0.0053,\n",
            "         0.0257,  0.0291, -0.0150, -0.0323,  0.0163, -0.0257,  0.0299, -0.0204,\n",
            "         0.0181, -0.0159, -0.0352,  0.0215, -0.0009, -0.0112, -0.0141, -0.0010,\n",
            "         0.0289,  0.0087,  0.0235,  0.0285,  0.0305,  0.0014,  0.0203, -0.0274,\n",
            "        -0.0350, -0.0194, -0.0048,  0.0343,  0.0007,  0.0132, -0.0224,  0.0192,\n",
            "        -0.0066, -0.0339, -0.0113,  0.0227], requires_grad=True))\n",
            "\n",
            "Parameter : ('l_2.weight', Parameter containing:\n",
            "tensor([[-4.9651e-02,  9.3507e-02,  2.0170e-02, -8.7704e-02,  4.4321e-02,\n",
            "         -8.6667e-02, -4.8988e-02,  4.6954e-02,  3.0037e-02,  5.9499e-02,\n",
            "         -3.0934e-02, -9.3036e-02,  6.7350e-02, -7.8849e-02, -2.9671e-02,\n",
            "         -2.3183e-02,  6.4995e-02,  2.7445e-02,  4.4316e-02, -8.2559e-02,\n",
            "          3.5323e-02,  6.2817e-02,  6.3581e-02,  6.5296e-02, -5.6013e-02,\n",
            "         -9.9433e-02,  6.5516e-03, -7.2514e-02,  2.2216e-02,  1.6778e-02,\n",
            "          8.2484e-02,  8.9192e-02, -9.5201e-02, -9.5092e-02, -2.4795e-02,\n",
            "         -8.8717e-02, -9.2845e-02,  7.4961e-02, -5.5886e-02,  8.4788e-02,\n",
            "         -2.7349e-02, -8.5188e-02,  3.0569e-02, -8.1645e-02, -3.5550e-02,\n",
            "          3.0566e-02,  9.1835e-03,  3.5019e-02,  9.4369e-02,  4.8488e-02,\n",
            "          1.5359e-02, -7.9523e-02,  6.9911e-03, -1.9679e-02,  6.6132e-02,\n",
            "          5.4858e-02, -9.0160e-02, -3.5423e-02,  7.4811e-02, -7.5448e-02,\n",
            "          6.3904e-02,  3.9194e-02,  1.0499e-02, -7.9876e-03,  7.8551e-03,\n",
            "          2.7012e-02,  3.4749e-02,  2.2469e-02,  6.3490e-02,  1.9635e-02,\n",
            "         -4.6124e-02, -4.7397e-02,  8.2582e-02,  1.7771e-02,  3.4285e-02,\n",
            "         -7.0422e-02,  5.8962e-02, -3.1754e-02,  1.3185e-02, -7.7829e-02,\n",
            "          6.6607e-02, -5.2293e-02,  1.1348e-02,  1.9973e-02, -6.7268e-02,\n",
            "         -2.5860e-02,  3.9768e-02, -8.2511e-03, -5.4690e-03, -5.6193e-03,\n",
            "          2.6682e-02, -7.1284e-02,  5.1115e-03, -4.0740e-02, -3.8940e-03,\n",
            "          8.9625e-02,  8.3371e-02,  4.9641e-03, -5.2269e-02, -7.7051e-02],\n",
            "        [-5.6843e-02,  3.4142e-04,  6.4219e-02,  5.3133e-02, -6.7221e-03,\n",
            "          2.9918e-02,  5.9856e-04,  6.6207e-02,  6.1344e-02, -2.1545e-02,\n",
            "          3.1530e-02,  1.6441e-02,  9.2280e-02,  2.5742e-02, -4.1211e-02,\n",
            "          3.2457e-02,  2.8142e-02,  9.0292e-02, -3.2125e-02,  8.2810e-02,\n",
            "          6.6217e-02,  8.4644e-02, -4.9701e-02,  9.6873e-02, -6.8766e-03,\n",
            "         -8.6112e-02,  7.0690e-02, -2.6138e-02, -7.1220e-02,  5.0436e-02,\n",
            "          1.5500e-02, -1.3093e-02,  5.4867e-02,  9.6708e-02,  4.1096e-02,\n",
            "         -4.8553e-02, -1.9761e-02, -4.1437e-02, -6.8491e-02,  7.8545e-02,\n",
            "          9.8598e-02, -6.5795e-02, -8.1347e-02, -3.7457e-02,  5.3427e-02,\n",
            "         -3.9427e-02, -4.0643e-02,  9.8184e-02,  2.2020e-02, -1.8495e-02,\n",
            "         -9.5384e-02,  6.8396e-02, -5.0477e-02,  8.4734e-03, -4.2798e-02,\n",
            "          5.1591e-02, -2.2252e-02, -5.1377e-02, -6.2719e-02, -1.7177e-02,\n",
            "         -6.0021e-03, -8.3642e-02,  2.6722e-02,  3.0393e-02, -9.3823e-02,\n",
            "         -1.6709e-02, -2.2208e-02,  1.1255e-02,  6.8394e-03,  9.5490e-02,\n",
            "         -1.8045e-02, -7.4525e-02,  2.1587e-02,  8.3510e-02,  4.9792e-02,\n",
            "          7.0849e-02, -2.2675e-02, -6.2321e-02, -7.1626e-02,  3.0602e-02,\n",
            "          2.4863e-02, -4.0577e-02, -9.4928e-04, -1.2568e-02, -7.6579e-02,\n",
            "         -7.5072e-02, -8.4889e-02, -4.5709e-02, -5.4284e-02, -2.2210e-02,\n",
            "         -6.2181e-02, -4.6304e-02, -6.6401e-02,  7.2337e-02, -3.3536e-03,\n",
            "          4.9435e-02,  3.0609e-02,  5.6447e-02, -7.5300e-02,  4.7867e-02],\n",
            "        [ 2.9972e-02,  5.6830e-02, -4.9692e-02,  3.7873e-02,  6.6369e-02,\n",
            "         -8.0551e-02,  5.0027e-02, -4.5054e-02, -3.8188e-03,  3.6819e-02,\n",
            "          8.9608e-02, -7.0930e-02,  8.2547e-02, -3.8564e-02,  4.6006e-02,\n",
            "         -6.1047e-02,  6.9684e-02, -4.5539e-02,  3.4669e-02, -1.2118e-02,\n",
            "          6.2124e-02, -7.6964e-02,  5.7722e-02,  6.2065e-02,  8.5439e-02,\n",
            "          5.5062e-02,  5.5990e-02,  8.2701e-02, -4.4307e-02, -7.0428e-02,\n",
            "          4.9583e-02,  4.7922e-02, -6.1538e-02,  8.2769e-02, -9.9963e-02,\n",
            "         -3.5043e-03,  8.0471e-02, -6.7161e-02, -1.5767e-02, -3.1660e-02,\n",
            "          7.6062e-02,  9.5010e-02, -5.8784e-02,  6.8540e-02,  5.6091e-02,\n",
            "          3.8594e-02,  5.2538e-03, -2.3441e-02,  5.1889e-02,  5.3327e-02,\n",
            "          2.3834e-02, -4.5312e-02,  8.6979e-02, -4.3168e-02,  1.2810e-02,\n",
            "          5.1558e-02, -7.9937e-02,  6.8495e-02,  7.8713e-02,  8.3792e-02,\n",
            "         -6.9866e-02,  9.2691e-02, -3.0739e-02, -3.1181e-02, -2.3551e-02,\n",
            "         -8.7973e-02,  7.0688e-02, -4.8766e-03, -4.8056e-03, -9.5113e-02,\n",
            "          5.4676e-02, -4.8634e-02, -4.5792e-02,  5.0996e-02, -3.8491e-02,\n",
            "          3.6610e-03,  7.5778e-03, -1.9176e-02,  7.8047e-02, -3.4820e-02,\n",
            "          4.9157e-02,  7.2717e-02, -3.7706e-02, -5.5897e-03, -3.7885e-02,\n",
            "          2.9402e-03, -7.3140e-02,  7.5868e-02, -6.2127e-02,  6.0259e-02,\n",
            "         -4.6135e-02,  4.7551e-02,  8.1952e-02, -6.5429e-02, -2.2791e-02,\n",
            "          8.4694e-02,  9.0744e-03,  2.6620e-03, -3.3120e-02,  8.6020e-02],\n",
            "        [ 1.7313e-02,  3.1840e-02,  3.8535e-02,  2.4547e-02,  1.8464e-02,\n",
            "          7.4431e-02, -9.2504e-02, -4.1909e-02,  7.0607e-02, -6.3434e-02,\n",
            "          7.9758e-02, -6.2301e-02, -6.3233e-03,  6.4649e-02, -7.2402e-02,\n",
            "         -3.0672e-02, -8.0872e-02, -9.2320e-03,  8.9954e-02,  4.9377e-02,\n",
            "         -1.3707e-02,  6.3765e-02, -7.9135e-03, -3.4836e-02, -2.5318e-02,\n",
            "          7.1123e-03, -2.3627e-02, -2.3565e-02,  1.7004e-02,  5.3870e-02,\n",
            "          9.7221e-02, -4.2008e-02, -2.3418e-02,  2.1085e-02,  3.0168e-03,\n",
            "         -9.6736e-02,  8.4284e-02, -7.3185e-02,  1.9697e-02,  4.7952e-02,\n",
            "          4.9907e-02, -2.6386e-02,  6.1469e-02, -2.7238e-02,  7.4385e-02,\n",
            "          4.4325e-03, -4.7509e-02, -6.6136e-02,  5.6767e-02,  1.4752e-03,\n",
            "         -2.3303e-02, -2.1240e-02,  3.2661e-02, -1.8582e-02,  3.9162e-02,\n",
            "         -6.5591e-02, -2.3155e-02, -2.6741e-02,  8.2299e-02,  2.4181e-02,\n",
            "          2.6451e-02,  9.1855e-02,  5.0090e-05, -7.5172e-02, -8.9801e-02,\n",
            "          3.4849e-02,  4.9229e-03,  5.1360e-03, -3.2921e-02,  2.0498e-02,\n",
            "         -8.7747e-02,  6.5000e-02,  1.0016e-02, -6.0353e-02, -5.4788e-03,\n",
            "          7.0645e-03, -5.5162e-02, -1.9536e-02,  2.3963e-03, -8.3205e-02,\n",
            "          8.1432e-02,  7.0932e-03, -7.2660e-02, -2.0323e-02, -3.5361e-02,\n",
            "         -3.2862e-02,  5.0282e-02, -2.7771e-02, -9.1135e-02,  5.3991e-02,\n",
            "          9.2151e-02,  9.5361e-02, -1.8093e-02, -7.2952e-03, -6.1262e-02,\n",
            "         -7.1391e-02, -5.7466e-02,  7.7867e-02,  8.2434e-02, -3.6593e-02],\n",
            "        [ 1.7374e-02,  1.7447e-02,  9.1701e-02, -3.4061e-02,  9.0221e-02,\n",
            "          3.7048e-02,  8.8102e-02, -2.3613e-03,  3.0379e-02,  9.2018e-02,\n",
            "         -5.7971e-02,  8.7035e-02,  7.6527e-02,  6.5619e-02,  2.5525e-02,\n",
            "          6.7903e-02, -6.9109e-02, -6.2192e-02,  8.4031e-02,  7.4883e-02,\n",
            "          5.6637e-02, -2.0414e-02,  1.6891e-02, -3.8049e-02, -7.4454e-02,\n",
            "         -5.6876e-02, -2.0765e-02,  9.9491e-02,  1.5320e-02, -8.2067e-02,\n",
            "          9.3417e-04, -9.0925e-02, -2.3832e-02, -9.5127e-03,  9.9719e-02,\n",
            "         -8.6155e-02,  6.3354e-02, -1.2555e-02,  9.4003e-02, -7.4575e-02,\n",
            "         -5.1549e-02, -9.5719e-02,  1.7518e-02, -2.4532e-02, -9.0503e-02,\n",
            "         -3.9509e-02, -6.2400e-02,  5.9410e-04, -4.8172e-02, -3.5974e-02,\n",
            "         -6.4363e-02, -5.7370e-02, -3.5090e-02,  8.4397e-02, -7.7600e-02,\n",
            "         -5.8137e-02, -9.0531e-02,  9.1455e-02,  8.0546e-02,  7.3253e-02,\n",
            "          3.9332e-02, -9.4816e-02, -3.4487e-02, -2.2946e-02,  3.2286e-02,\n",
            "          3.9435e-02,  6.0954e-02,  7.9588e-02, -6.6326e-02,  2.7010e-02,\n",
            "         -7.8426e-02, -1.9333e-02, -6.0606e-02,  1.0603e-02,  4.7102e-02,\n",
            "          7.2644e-02, -1.3355e-02,  1.7948e-02, -9.5608e-02,  7.3827e-02,\n",
            "         -2.8032e-02,  8.9329e-02, -4.0909e-02, -4.1498e-02, -4.2367e-02,\n",
            "         -9.3965e-03, -8.0948e-02,  6.3994e-03, -1.6669e-02, -8.7559e-02,\n",
            "          2.0269e-02, -4.8383e-02,  7.5242e-02,  2.6621e-02,  9.6873e-02,\n",
            "          3.4225e-02,  8.0351e-02,  9.5091e-02, -6.1129e-03, -5.1665e-02],\n",
            "        [ 6.5238e-02, -9.0840e-02, -5.8325e-02,  4.6664e-02, -5.8527e-02,\n",
            "          9.0943e-02,  4.5813e-02,  5.3839e-03, -3.9745e-02,  8.3194e-02,\n",
            "         -6.3995e-02, -5.6166e-02,  9.1574e-02, -9.6673e-02,  7.0998e-02,\n",
            "          5.8270e-02,  6.8626e-02, -7.7096e-02, -8.8686e-02,  7.8335e-02,\n",
            "         -9.9825e-02, -9.4769e-02,  2.3746e-02,  7.4000e-02, -7.9742e-02,\n",
            "          1.3895e-02, -2.6757e-02,  6.0276e-02, -2.6626e-02,  8.3616e-03,\n",
            "         -6.1248e-03, -2.5159e-02,  4.4052e-02,  1.2930e-02, -8.4147e-02,\n",
            "          7.7976e-02, -9.9207e-02, -3.9763e-02,  8.7471e-02, -1.2331e-02,\n",
            "         -8.8377e-02, -9.4833e-02, -6.8176e-02, -1.5732e-02, -4.3222e-02,\n",
            "         -8.9065e-02,  9.1130e-02,  2.5110e-02,  9.3573e-04, -5.2547e-02,\n",
            "         -8.6840e-02, -2.7627e-02, -4.1851e-03, -9.7736e-02, -3.8758e-02,\n",
            "         -4.0916e-02,  2.8907e-03,  5.8538e-02,  9.7150e-02,  1.2419e-02,\n",
            "         -3.8558e-02,  8.3301e-02,  6.7825e-02, -6.1192e-02,  9.8048e-02,\n",
            "          1.9001e-02, -5.0157e-03, -2.8102e-02,  1.6143e-02, -8.4860e-02,\n",
            "         -9.9701e-02,  4.7362e-02,  7.8861e-02,  6.3801e-02,  3.7513e-02,\n",
            "          8.5668e-02, -2.5063e-02,  6.5725e-03, -2.6289e-02, -9.8554e-02,\n",
            "         -2.3953e-02,  4.9225e-03, -6.0976e-02,  6.0691e-02,  6.3120e-02,\n",
            "          6.8362e-02,  9.9561e-02, -4.8132e-02,  4.8635e-02, -2.6424e-03,\n",
            "          2.9832e-02, -4.5930e-02, -8.4316e-02,  1.3931e-02,  4.0440e-02,\n",
            "         -7.1765e-02,  8.2012e-02, -2.7516e-02, -5.4033e-02,  8.2721e-02],\n",
            "        [ 5.0695e-02, -2.8722e-03,  4.8655e-03, -8.1142e-02, -7.2862e-02,\n",
            "          9.1014e-02,  3.7333e-02,  2.4859e-02,  1.9373e-02, -4.3810e-03,\n",
            "         -3.7619e-02, -1.8233e-02,  8.1874e-05,  9.6022e-02,  5.7363e-02,\n",
            "         -9.0939e-02,  5.0885e-02, -7.8191e-02,  9.2795e-02, -1.2441e-02,\n",
            "         -1.2497e-02,  7.2060e-02, -6.8630e-02,  1.3071e-02,  3.9275e-02,\n",
            "         -7.7210e-02,  4.5492e-03, -6.3217e-02,  6.6434e-02, -9.0379e-02,\n",
            "          1.7905e-03, -4.1327e-02,  3.7001e-02, -7.8747e-02, -6.4873e-03,\n",
            "          4.8132e-02, -2.8998e-02, -7.0600e-02,  2.1273e-02, -5.0881e-03,\n",
            "          9.3802e-02,  4.2572e-02, -9.1266e-02, -7.3766e-02,  5.8956e-02,\n",
            "         -9.3111e-02,  9.8007e-02, -1.1982e-02, -7.7231e-02, -9.5467e-02,\n",
            "         -4.5215e-02,  9.3418e-02, -3.7380e-02, -9.8124e-02,  4.8748e-02,\n",
            "          4.1982e-03,  6.0002e-02, -9.3112e-02,  6.3838e-02,  8.6153e-02,\n",
            "          8.3153e-02,  7.2878e-02, -1.4989e-02,  7.0794e-03,  6.3579e-02,\n",
            "          5.0891e-02,  9.8724e-02,  2.8120e-02,  1.6412e-02, -7.1717e-02,\n",
            "         -6.7888e-02,  6.4620e-02, -1.4265e-03, -8.3954e-02, -4.5554e-02,\n",
            "         -3.5888e-02, -4.5909e-02,  6.5431e-02, -4.1272e-02,  7.2157e-02,\n",
            "          4.6957e-03,  2.3154e-04,  2.5692e-02,  1.0899e-02,  4.8495e-02,\n",
            "          7.3284e-02, -9.5514e-02,  5.5414e-02, -6.6378e-02, -3.6724e-02,\n",
            "         -8.0074e-02, -5.8353e-02, -7.0678e-02,  6.0439e-02, -4.4136e-02,\n",
            "         -1.4499e-02, -7.0495e-02,  1.5045e-02, -4.9537e-02, -4.3222e-02],\n",
            "        [ 7.6838e-02, -6.2769e-02, -5.7634e-02, -7.5193e-03, -4.2936e-02,\n",
            "          1.7064e-02,  3.1926e-02,  7.9181e-02, -6.1538e-02,  2.8337e-02,\n",
            "         -9.4476e-02,  7.0212e-02, -3.9503e-02, -1.1936e-02, -1.2383e-02,\n",
            "         -2.7011e-02,  3.8416e-02,  2.3921e-02,  7.2718e-02, -4.5931e-02,\n",
            "          6.0124e-02,  2.2357e-03,  4.3946e-02, -1.1540e-02,  3.2442e-02,\n",
            "         -5.4852e-02, -9.9425e-03, -2.9757e-02,  8.6767e-02, -9.7906e-02,\n",
            "          4.0536e-02, -3.9011e-02,  5.4897e-02, -7.0208e-02, -8.2517e-02,\n",
            "          9.7026e-02, -4.2319e-02,  3.1969e-03,  2.1982e-02, -3.5373e-02,\n",
            "         -2.9073e-02,  7.7259e-02, -1.5928e-02,  8.5018e-03,  1.4766e-02,\n",
            "          4.6025e-02, -8.8925e-03,  9.2148e-02, -1.5458e-02, -9.9637e-02,\n",
            "         -1.5182e-02,  6.3362e-02,  7.4653e-02,  4.9286e-02, -6.8458e-03,\n",
            "          3.7121e-02,  6.6999e-02,  7.9777e-02,  3.4493e-02, -6.2578e-02,\n",
            "         -9.6703e-02, -6.9517e-02,  9.3396e-02, -3.8197e-02, -6.0369e-02,\n",
            "         -5.3908e-02, -4.6383e-03, -2.3219e-03,  5.2462e-02,  2.3601e-02,\n",
            "         -5.1420e-02, -3.8520e-02,  5.3013e-02, -5.4241e-02, -7.5124e-02,\n",
            "          8.5494e-02,  7.6579e-02, -9.7233e-02, -1.1199e-03, -5.1345e-02,\n",
            "          8.7474e-02,  4.7346e-02, -5.0747e-02, -6.4343e-02, -6.9030e-02,\n",
            "         -4.6935e-02, -2.7937e-03,  5.7623e-03, -7.1567e-02,  3.7992e-02,\n",
            "          6.1365e-02,  8.0542e-02, -9.7577e-02, -7.0304e-02,  2.2443e-03,\n",
            "         -3.4203e-02, -5.9361e-02,  2.2846e-02,  4.5033e-04, -3.5048e-02],\n",
            "        [ 5.6711e-02, -7.9930e-02,  2.3559e-02,  2.4453e-02, -8.8467e-02,\n",
            "          4.8271e-02,  8.7129e-02, -6.4965e-02, -8.5889e-02,  7.8447e-02,\n",
            "          9.4721e-02,  8.5771e-02,  6.6281e-03, -5.7029e-02,  5.7724e-02,\n",
            "         -5.4399e-02,  2.7130e-02,  9.9247e-02,  7.4394e-02,  4.0139e-02,\n",
            "         -9.9941e-03, -3.9100e-02,  1.4424e-02, -7.4668e-02,  7.5464e-02,\n",
            "         -6.2745e-03,  9.0343e-02,  5.4187e-02, -7.1050e-02, -8.5809e-02,\n",
            "          7.3143e-02, -9.6288e-02,  1.8075e-02, -5.4575e-02, -5.1179e-02,\n",
            "          7.1060e-02,  3.3507e-02, -6.0634e-02,  1.9432e-02,  8.8830e-02,\n",
            "          2.5363e-02, -4.2572e-02, -3.4493e-02, -2.4369e-04, -5.8983e-02,\n",
            "          5.1152e-02,  7.1839e-02,  5.5195e-02,  2.5097e-02,  8.3815e-02,\n",
            "          9.7527e-02,  5.1168e-02,  8.3773e-02, -8.9876e-02, -6.1561e-03,\n",
            "         -9.2988e-02,  9.9284e-02, -5.6311e-02,  9.0143e-02,  7.2868e-02,\n",
            "          3.6952e-02,  9.1723e-02, -8.3811e-02,  1.3326e-03,  8.1209e-02,\n",
            "          2.2729e-02, -9.7188e-02,  4.8046e-02, -4.4237e-02, -5.8930e-02,\n",
            "         -5.3283e-02,  4.9758e-02,  8.1197e-02,  5.6625e-02,  1.8356e-02,\n",
            "          9.9405e-02, -9.8437e-03,  4.0599e-03, -3.4770e-02,  7.5275e-02,\n",
            "          3.3914e-03, -1.5611e-02,  6.9392e-02,  7.6579e-02,  7.5314e-02,\n",
            "         -4.2637e-03, -2.6257e-02,  1.5247e-02, -9.0416e-02,  1.4592e-02,\n",
            "          4.3081e-02, -4.4332e-02, -7.1948e-02,  1.2153e-02,  4.1054e-02,\n",
            "          5.8074e-02, -1.9474e-02, -4.3920e-02, -9.6007e-02,  1.9536e-02],\n",
            "        [-4.1398e-02,  6.5460e-02,  3.8529e-02, -6.2987e-02, -7.2908e-02,\n",
            "          2.8455e-02,  4.4274e-02, -7.7357e-02,  2.0289e-03,  7.5651e-02,\n",
            "         -6.2847e-02,  9.5531e-02, -3.6649e-02,  7.7849e-02,  2.6237e-02,\n",
            "         -6.5746e-02,  1.2778e-02, -6.7195e-02,  3.1887e-02, -7.2690e-02,\n",
            "          1.0885e-03, -8.4075e-02, -8.3193e-03,  7.6099e-02, -1.9043e-02,\n",
            "         -6.2030e-02,  8.4750e-02,  8.6199e-02, -8.2493e-02, -7.0673e-03,\n",
            "          2.9119e-02, -3.4861e-02,  1.9572e-02, -3.0781e-02, -2.9848e-02,\n",
            "          5.2871e-02,  5.2160e-02, -2.9853e-02,  7.3942e-02,  4.1768e-02,\n",
            "         -5.2930e-02,  1.1418e-02, -6.1913e-02,  5.1332e-02,  7.1034e-02,\n",
            "         -9.0319e-02,  3.8317e-02, -9.1457e-02, -8.7261e-02, -3.9177e-02,\n",
            "          7.6200e-02,  5.5965e-02, -9.4632e-03,  6.4120e-02, -6.8189e-02,\n",
            "         -5.5931e-02, -4.7628e-02,  4.4919e-02, -5.9847e-02, -9.5380e-02,\n",
            "         -5.1957e-02,  9.3467e-02,  5.1879e-02, -8.2753e-03,  2.3818e-02,\n",
            "          8.7069e-02,  8.9120e-02, -2.3031e-02, -8.0473e-02, -1.9120e-02,\n",
            "         -9.1890e-03, -1.8646e-02,  5.9803e-02,  8.4078e-02,  4.8826e-02,\n",
            "         -3.3380e-02,  1.6093e-02, -1.7764e-02, -2.4358e-02,  1.6865e-02,\n",
            "          6.4130e-02,  6.7125e-02, -1.5646e-02,  9.8876e-02, -6.4178e-02,\n",
            "          1.7490e-02,  3.3713e-02, -2.6771e-02, -9.7555e-02, -3.6405e-02,\n",
            "         -9.4574e-02,  5.9816e-02, -1.8313e-02, -2.3949e-02, -8.4696e-02,\n",
            "         -3.6490e-02,  4.2796e-02, -1.6252e-02, -8.0398e-02,  7.2920e-02]],\n",
            "       requires_grad=True))\n",
            "\n",
            "Parameter : ('l_2.bias', Parameter containing:\n",
            "tensor([-0.0801,  0.0913,  0.0978,  0.0749, -0.0486,  0.0719,  0.0241, -0.0103,\n",
            "         0.0921, -0.0544], requires_grad=True))\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# data samples shape\n",
        "\n",
        "# custom class model (linear regression layer)\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size,hidden_size, num_classes) -> None:\n",
        "       super().__init__()\n",
        "       self.in_size = input_size\n",
        "       self.hidden = hidden_size\n",
        "       self.num_classes = num_classes\n",
        "       \n",
        "       # Arquitecture of FFN\n",
        "       self.l_1 = nn.Linear(in_features= self.in_size, out_features = self.hidden, bias = True)\n",
        "       self.Relu = nn.ReLU()\n",
        "       self.l_2 = nn.Linear(in_features=  self.hidden, out_features = self.num_classes, bias = True)\n",
        "       \n",
        "    def forward(self,x):\n",
        "        return self.l_2(self.Relu(self.l_1(x)))\n",
        "    \n",
        "\n",
        "# instanciar capa y crear modelo\n",
        "model = NeuralNet(input_size =input_size ,hidden_size = hidden_size, num_classes = num_clases)\n",
        "# loss class\n",
        "loss = nn.CrossEntropyLoss()\n",
        "# crea optimizer para prescendir de actualizacion de pesos a mano\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
        "\n",
        "# MODULE CLASS DOCU:\n",
        "\"\"\"nn.Module :\n",
        "Base class for all neural network modules.\n",
        "\n",
        "Your models should also subclass this class.\n",
        "\n",
        "Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes: \n",
        "self.sub_module = nn.Linear(...)\"\"\"\n",
        "\n",
        "# Iterate along all de modules inside a network class or model class. Notice that LinearRegression module has inside a linear layer \"module\" or only linear layer\n",
        "# note that the atribute name : self.linear will define the string \"linear\" to refer to that layer inside a module\n",
        "# this will be useful when ,inside a class module, there are several layers\n",
        "print(\"\\n\")\n",
        "\n",
        "for m in model.modules():\n",
        "    print(f\"\\nModule : {m}\")\n",
        "\n",
        "# iterate along all the parameters inside a module ( a module can have a lot of layers with their parameters)\n",
        "for p in model.named_parameters(prefix='', recurse=True, remove_duplicate=True):\n",
        "    print(f\"\\nParameter : {p}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2neB6EJOLhpB",
        "outputId": "46ce8dd7-b529-4b84-f56c-6d5228a437f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 2.295\n",
            "epoch : 0\n",
            "loss : 2.287\n",
            "epoch : 0\n",
            "loss : 2.208\n",
            "epoch : 0\n",
            "loss : 2.221\n",
            "epoch : 0\n",
            "loss : 2.134\n",
            "epoch : 0\n",
            "loss : 2.103\n",
            "epoch : 0\n",
            "loss : 2.036\n",
            "epoch : 0\n",
            "loss : 2.082\n",
            "epoch : 0\n",
            "loss : 2.036\n",
            "epoch : 0\n",
            "loss : 1.917\n",
            "epoch : 0\n",
            "loss : 1.916\n",
            "epoch : 0\n",
            "loss : 1.845\n",
            "epoch : 0\n",
            "loss : 1.770\n",
            "epoch : 0\n",
            "loss : 1.801\n",
            "epoch : 0\n",
            "loss : 1.774\n",
            "epoch : 0\n",
            "loss : 1.719\n",
            "epoch : 0\n",
            "loss : 1.661\n",
            "epoch : 0\n",
            "loss : 1.588\n",
            "epoch : 0\n",
            "loss : 1.451\n",
            "epoch : 0\n",
            "loss : 1.435\n",
            "epoch : 0\n",
            "loss : 1.516\n",
            "epoch : 0\n",
            "loss : 1.399\n",
            "epoch : 0\n",
            "loss : 1.421\n",
            "epoch : 0\n",
            "loss : 1.267\n",
            "epoch : 0\n",
            "loss : 1.267\n",
            "epoch : 0\n",
            "loss : 1.382\n",
            "epoch : 0\n",
            "loss : 1.239\n",
            "epoch : 0\n",
            "loss : 1.149\n",
            "epoch : 0\n",
            "loss : 1.234\n",
            "epoch : 0\n",
            "loss : 1.083\n",
            "epoch : 0\n",
            "loss : 1.029\n",
            "epoch : 0\n",
            "loss : 1.103\n",
            "epoch : 0\n",
            "loss : 1.036\n",
            "epoch : 0\n",
            "loss : 1.053\n",
            "epoch : 0\n",
            "loss : 0.963\n",
            "epoch : 0\n",
            "loss : 0.980\n",
            "epoch : 0\n",
            "loss : 0.926\n",
            "epoch : 0\n",
            "loss : 0.867\n",
            "epoch : 0\n",
            "loss : 0.787\n",
            "epoch : 0\n",
            "loss : 0.865\n",
            "epoch : 0\n",
            "loss : 0.875\n",
            "epoch : 0\n",
            "loss : 0.791\n",
            "epoch : 0\n",
            "loss : 0.797\n",
            "epoch : 0\n",
            "loss : 0.719\n",
            "epoch : 0\n",
            "loss : 0.749\n",
            "epoch : 0\n",
            "loss : 0.734\n",
            "epoch : 0\n",
            "loss : 0.718\n",
            "epoch : 0\n",
            "loss : 0.669\n",
            "epoch : 0\n",
            "loss : 0.759\n",
            "epoch : 0\n",
            "loss : 0.558\n",
            "epoch : 0\n",
            "loss : 0.694\n",
            "epoch : 0\n",
            "loss : 0.568\n",
            "epoch : 0\n",
            "loss : 0.652\n",
            "epoch : 0\n",
            "loss : 0.707\n",
            "epoch : 0\n",
            "loss : 0.512\n",
            "epoch : 0\n",
            "loss : 0.683\n",
            "epoch : 0\n",
            "loss : 0.625\n",
            "epoch : 0\n",
            "loss : 0.565\n",
            "epoch : 0\n",
            "loss : 0.511\n",
            "epoch : 0\n",
            "loss : 0.609\n",
            "epoch : 0\n",
            "loss : 0.565\n",
            "epoch : 0\n",
            "loss : 0.529\n",
            "epoch : 0\n",
            "loss : 0.549\n",
            "epoch : 0\n",
            "loss : 0.531\n",
            "epoch : 0\n",
            "loss : 0.583\n",
            "epoch : 0\n",
            "loss : 0.586\n",
            "epoch : 0\n",
            "loss : 0.641\n",
            "epoch : 0\n",
            "loss : 0.535\n",
            "epoch : 0\n",
            "loss : 0.429\n",
            "epoch : 0\n",
            "loss : 0.486\n",
            "epoch : 0\n",
            "loss : 0.559\n",
            "epoch : 0\n",
            "loss : 0.428\n",
            "epoch : 0\n",
            "loss : 0.530\n",
            "epoch : 0\n",
            "loss : 0.609\n",
            "epoch : 0\n",
            "loss : 0.464\n",
            "epoch : 0\n",
            "loss : 0.404\n",
            "epoch : 0\n",
            "loss : 0.531\n",
            "epoch : 0\n",
            "loss : 0.421\n",
            "epoch : 0\n",
            "loss : 0.469\n",
            "epoch : 0\n",
            "loss : 0.474\n",
            "epoch : 0\n",
            "loss : 0.419\n",
            "epoch : 0\n",
            "loss : 0.533\n",
            "epoch : 0\n",
            "loss : 0.343\n",
            "epoch : 0\n",
            "loss : 0.535\n",
            "epoch : 0\n",
            "loss : 0.343\n",
            "epoch : 0\n",
            "loss : 0.406\n",
            "epoch : 0\n",
            "loss : 0.433\n",
            "epoch : 0\n",
            "loss : 0.374\n",
            "epoch : 0\n",
            "loss : 0.494\n",
            "epoch : 0\n",
            "loss : 0.493\n",
            "epoch : 0\n",
            "loss : 0.413\n",
            "epoch : 0\n",
            "loss : 0.404\n",
            "epoch : 0\n",
            "loss : 0.410\n",
            "epoch : 0\n",
            "loss : 0.369\n",
            "epoch : 0\n",
            "loss : 0.459\n",
            "epoch : 0\n",
            "loss : 0.471\n",
            "epoch : 0\n",
            "loss : 0.484\n",
            "epoch : 0\n",
            "loss : 0.416\n",
            "epoch : 0\n",
            "loss : 0.442\n",
            "epoch : 0\n",
            "loss : 0.401\n",
            "epoch : 0\n",
            "loss : 0.511\n",
            "epoch : 0\n",
            "loss : 0.480\n",
            "epoch : 0\n",
            "loss : 0.491\n",
            "epoch : 0\n",
            "loss : 0.294\n",
            "epoch : 0\n",
            "loss : 0.313\n",
            "epoch : 0\n",
            "loss : 0.442\n",
            "epoch : 0\n",
            "loss : 0.459\n",
            "epoch : 0\n",
            "loss : 0.336\n",
            "epoch : 0\n",
            "loss : 0.419\n",
            "epoch : 0\n",
            "loss : 0.434\n",
            "epoch : 0\n",
            "loss : 0.496\n",
            "epoch : 0\n",
            "loss : 0.519\n",
            "epoch : 0\n",
            "loss : 0.413\n",
            "epoch : 0\n",
            "loss : 0.375\n",
            "epoch : 0\n",
            "loss : 0.384\n",
            "epoch : 0\n",
            "loss : 0.500\n",
            "epoch : 0\n",
            "loss : 0.390\n",
            "epoch : 0\n",
            "loss : 0.566\n",
            "epoch : 0\n",
            "loss : 0.394\n",
            "epoch : 0\n",
            "loss : 0.581\n",
            "epoch : 0\n",
            "loss : 0.388\n",
            "epoch : 0\n",
            "loss : 0.358\n",
            "epoch : 0\n",
            "loss : 0.322\n",
            "epoch : 0\n",
            "loss : 0.308\n",
            "epoch : 0\n",
            "loss : 0.404\n",
            "epoch : 0\n",
            "loss : 0.405\n",
            "epoch : 0\n",
            "loss : 0.440\n",
            "epoch : 0\n",
            "loss : 0.439\n",
            "epoch : 0\n",
            "loss : 0.589\n",
            "epoch : 0\n",
            "loss : 0.451\n",
            "epoch : 0\n",
            "loss : 0.414\n",
            "epoch : 0\n",
            "loss : 0.381\n",
            "epoch : 0\n",
            "loss : 0.445\n",
            "epoch : 0\n",
            "loss : 0.406\n",
            "epoch : 0\n",
            "loss : 0.411\n",
            "epoch : 0\n",
            "loss : 0.350\n",
            "epoch : 0\n",
            "loss : 0.385\n",
            "epoch : 0\n",
            "loss : 0.493\n",
            "epoch : 0\n",
            "loss : 0.440\n",
            "epoch : 0\n",
            "loss : 0.339\n",
            "epoch : 0\n",
            "loss : 0.395\n",
            "epoch : 0\n",
            "loss : 0.395\n",
            "epoch : 0\n",
            "loss : 0.296\n",
            "epoch : 0\n",
            "loss : 0.328\n",
            "epoch : 0\n",
            "loss : 0.361\n",
            "epoch : 0\n",
            "loss : 0.268\n",
            "epoch : 0\n",
            "loss : 0.377\n",
            "epoch : 0\n",
            "loss : 0.436\n",
            "epoch : 0\n",
            "loss : 0.287\n",
            "epoch : 0\n",
            "loss : 0.348\n",
            "epoch : 0\n",
            "loss : 0.302\n",
            "epoch : 0\n",
            "loss : 0.340\n",
            "epoch : 0\n",
            "loss : 0.223\n",
            "epoch : 0\n",
            "loss : 0.302\n",
            "epoch : 0\n",
            "loss : 0.368\n",
            "epoch : 0\n",
            "loss : 0.394\n",
            "epoch : 0\n",
            "loss : 0.303\n",
            "epoch : 0\n",
            "loss : 0.299\n",
            "epoch : 0\n",
            "loss : 0.329\n",
            "epoch : 0\n",
            "loss : 0.268\n",
            "epoch : 0\n",
            "loss : 0.379\n",
            "epoch : 0\n",
            "loss : 0.284\n",
            "epoch : 0\n",
            "loss : 0.388\n",
            "epoch : 0\n",
            "loss : 0.500\n",
            "epoch : 0\n",
            "loss : 0.380\n",
            "epoch : 0\n",
            "loss : 0.309\n",
            "epoch : 0\n",
            "loss : 0.340\n",
            "epoch : 0\n",
            "loss : 0.326\n",
            "epoch : 0\n",
            "loss : 0.506\n",
            "epoch : 0\n",
            "loss : 0.219\n",
            "epoch : 0\n",
            "loss : 0.441\n",
            "epoch : 0\n",
            "loss : 0.456\n",
            "epoch : 0\n",
            "loss : 0.308\n",
            "epoch : 0\n",
            "loss : 0.366\n",
            "epoch : 0\n",
            "loss : 0.325\n",
            "epoch : 0\n",
            "loss : 0.374\n",
            "epoch : 0\n",
            "loss : 0.381\n",
            "epoch : 0\n",
            "loss : 0.355\n",
            "epoch : 0\n",
            "loss : 0.320\n",
            "epoch : 0\n",
            "loss : 0.427\n",
            "epoch : 0\n",
            "loss : 0.373\n",
            "epoch : 0\n",
            "loss : 0.354\n",
            "epoch : 0\n",
            "loss : 0.444\n",
            "epoch : 0\n",
            "loss : 0.337\n",
            "epoch : 0\n",
            "loss : 0.312\n",
            "epoch : 0\n",
            "loss : 0.374\n",
            "epoch : 0\n",
            "loss : 0.300\n",
            "epoch : 0\n",
            "loss : 0.602\n",
            "epoch : 0\n",
            "loss : 0.382\n",
            "epoch : 0\n",
            "loss : 0.356\n",
            "epoch : 0\n",
            "loss : 0.183\n",
            "epoch : 0\n",
            "loss : 0.458\n",
            "epoch : 0\n",
            "loss : 0.326\n",
            "epoch : 0\n",
            "loss : 0.495\n",
            "epoch : 0\n",
            "loss : 0.292\n",
            "epoch : 0\n",
            "loss : 0.331\n",
            "epoch : 0\n",
            "loss : 0.405\n",
            "epoch : 0\n",
            "loss : 0.294\n",
            "epoch : 0\n",
            "loss : 0.303\n",
            "epoch : 0\n",
            "loss : 0.298\n",
            "epoch : 0\n",
            "loss : 0.268\n",
            "epoch : 0\n",
            "loss : 0.413\n",
            "epoch : 0\n",
            "loss : 0.264\n",
            "epoch : 0\n",
            "loss : 0.400\n",
            "epoch : 0\n",
            "loss : 0.369\n",
            "epoch : 0\n",
            "loss : 0.263\n",
            "epoch : 0\n",
            "loss : 0.290\n",
            "epoch : 0\n",
            "loss : 0.268\n",
            "epoch : 0\n",
            "loss : 0.239\n",
            "epoch : 0\n",
            "loss : 0.360\n",
            "epoch : 0\n",
            "loss : 0.211\n",
            "epoch : 0\n",
            "loss : 0.268\n",
            "epoch : 0\n",
            "loss : 0.329\n",
            "epoch : 0\n",
            "loss : 0.344\n",
            "epoch : 0\n",
            "loss : 0.351\n",
            "epoch : 0\n",
            "loss : 0.360\n",
            "epoch : 0\n",
            "loss : 0.429\n",
            "epoch : 0\n",
            "loss : 0.406\n",
            "epoch : 0\n",
            "loss : 0.316\n",
            "epoch : 0\n",
            "loss : 0.475\n",
            "epoch : 0\n",
            "loss : 0.426\n",
            "epoch : 0\n",
            "loss : 0.355\n",
            "epoch : 0\n",
            "loss : 0.483\n",
            "epoch : 0\n",
            "loss : 0.310\n",
            "epoch : 0\n",
            "loss : 0.301\n",
            "epoch : 0\n",
            "loss : 0.329\n",
            "epoch : 0\n",
            "loss : 0.450\n",
            "epoch : 0\n",
            "loss : 0.508\n",
            "epoch : 0\n",
            "loss : 0.202\n",
            "epoch : 0\n",
            "loss : 0.420\n",
            "epoch : 0\n",
            "loss : 0.328\n",
            "epoch : 0\n",
            "loss : 0.257\n",
            "epoch : 0\n",
            "loss : 0.344\n",
            "epoch : 0\n",
            "loss : 0.226\n",
            "epoch : 0\n",
            "loss : 0.505\n",
            "epoch : 0\n",
            "loss : 0.418\n",
            "epoch : 0\n",
            "loss : 0.372\n",
            "epoch : 0\n",
            "loss : 0.311\n",
            "epoch : 0\n",
            "loss : 0.441\n",
            "epoch : 0\n",
            "loss : 0.306\n",
            "epoch : 0\n",
            "loss : 0.299\n",
            "epoch : 0\n",
            "loss : 0.254\n",
            "epoch : 0\n",
            "loss : 0.321\n",
            "epoch : 0\n",
            "loss : 0.227\n",
            "epoch : 0\n",
            "loss : 0.362\n",
            "epoch : 0\n",
            "loss : 0.203\n",
            "epoch : 0\n",
            "loss : 0.299\n",
            "epoch : 0\n",
            "loss : 0.297\n",
            "epoch : 0\n",
            "loss : 0.289\n",
            "epoch : 0\n",
            "loss : 0.253\n",
            "epoch : 0\n",
            "loss : 0.391\n",
            "epoch : 0\n",
            "loss : 0.323\n",
            "epoch : 0\n",
            "loss : 0.255\n",
            "epoch : 0\n",
            "loss : 0.418\n",
            "epoch : 0\n",
            "loss : 0.309\n",
            "epoch : 0\n",
            "loss : 0.156\n",
            "epoch : 0\n",
            "loss : 0.358\n",
            "epoch : 0\n",
            "loss : 0.276\n",
            "epoch : 0\n",
            "loss : 0.299\n",
            "epoch : 0\n",
            "loss : 0.405\n",
            "epoch : 0\n",
            "loss : 0.290\n",
            "epoch : 0\n",
            "loss : 0.288\n",
            "epoch : 0\n",
            "loss : 0.156\n",
            "epoch : 0\n",
            "loss : 0.307\n",
            "epoch : 0\n",
            "loss : 0.525\n",
            "epoch : 0\n",
            "loss : 0.378\n",
            "epoch : 0\n",
            "loss : 0.269\n",
            "epoch : 0\n",
            "loss : 0.310\n",
            "epoch : 0\n",
            "loss : 0.325\n",
            "epoch : 0\n",
            "loss : 0.238\n",
            "epoch : 0\n",
            "loss : 0.344\n",
            "epoch : 0\n",
            "loss : 0.232\n",
            "epoch : 0\n",
            "loss : 0.497\n",
            "epoch : 0\n",
            "loss : 0.312\n",
            "epoch : 0\n",
            "loss : 0.451\n",
            "epoch : 0\n",
            "loss : 0.314\n",
            "epoch : 0\n",
            "loss : 0.317\n",
            "epoch : 0\n",
            "loss : 0.210\n",
            "epoch : 0\n",
            "loss : 0.345\n",
            "epoch : 0\n",
            "loss : 0.241\n",
            "epoch : 0\n",
            "loss : 0.286\n",
            "epoch : 0\n",
            "loss : 0.304\n",
            "epoch : 0\n",
            "loss : 0.351\n",
            "epoch : 0\n",
            "loss : 0.222\n",
            "epoch : 0\n",
            "loss : 0.355\n",
            "epoch : 0\n",
            "loss : 0.224\n",
            "epoch : 0\n",
            "loss : 0.340\n",
            "epoch : 0\n",
            "loss : 0.327\n",
            "epoch : 0\n",
            "loss : 0.227\n",
            "epoch : 0\n",
            "loss : 0.330\n",
            "epoch : 0\n",
            "loss : 0.350\n",
            "epoch : 0\n",
            "loss : 0.370\n",
            "epoch : 0\n",
            "loss : 0.334\n",
            "epoch : 0\n",
            "loss : 0.252\n",
            "epoch : 0\n",
            "loss : 0.430\n",
            "epoch : 0\n",
            "loss : 0.305\n",
            "epoch : 0\n",
            "loss : 0.200\n",
            "epoch : 0\n",
            "loss : 0.225\n",
            "epoch : 0\n",
            "loss : 0.227\n",
            "epoch : 0\n",
            "loss : 0.323\n",
            "epoch : 0\n",
            "loss : 0.371\n",
            "epoch : 0\n",
            "loss : 0.241\n",
            "epoch : 0\n",
            "loss : 0.286\n",
            "epoch : 0\n",
            "loss : 0.337\n",
            "epoch : 0\n",
            "loss : 0.215\n",
            "epoch : 0\n",
            "loss : 0.304\n",
            "epoch : 0\n",
            "loss : 0.353\n",
            "epoch : 0\n",
            "loss : 0.464\n",
            "epoch : 0\n",
            "loss : 0.459\n",
            "epoch : 0\n",
            "loss : 0.335\n",
            "epoch : 0\n",
            "loss : 0.353\n",
            "epoch : 0\n",
            "loss : 0.426\n",
            "epoch : 0\n",
            "loss : 0.342\n",
            "epoch : 0\n",
            "loss : 0.335\n",
            "epoch : 0\n",
            "loss : 0.182\n",
            "epoch : 0\n",
            "loss : 0.235\n",
            "epoch : 0\n",
            "loss : 0.431\n",
            "epoch : 0\n",
            "loss : 0.260\n",
            "epoch : 0\n",
            "loss : 0.300\n",
            "epoch : 0\n",
            "loss : 0.211\n",
            "epoch : 0\n",
            "loss : 0.242\n",
            "epoch : 0\n",
            "loss : 0.307\n",
            "epoch : 0\n",
            "loss : 0.161\n",
            "epoch : 0\n",
            "loss : 0.425\n",
            "epoch : 0\n",
            "loss : 0.313\n",
            "epoch : 0\n",
            "loss : 0.242\n",
            "epoch : 0\n",
            "loss : 0.302\n",
            "epoch : 0\n",
            "loss : 0.367\n",
            "epoch : 0\n",
            "loss : 0.293\n",
            "epoch : 0\n",
            "loss : 0.211\n",
            "epoch : 0\n",
            "loss : 0.215\n",
            "epoch : 0\n",
            "loss : 0.248\n",
            "epoch : 0\n",
            "loss : 0.384\n",
            "epoch : 0\n",
            "loss : 0.271\n",
            "epoch : 0\n",
            "loss : 0.139\n",
            "epoch : 0\n",
            "loss : 0.239\n",
            "epoch : 0\n",
            "loss : 0.159\n",
            "epoch : 0\n",
            "loss : 0.213\n",
            "epoch : 0\n",
            "loss : 0.364\n",
            "epoch : 0\n",
            "loss : 0.353\n",
            "epoch : 0\n",
            "loss : 0.345\n",
            "epoch : 0\n",
            "loss : 0.234\n",
            "epoch : 0\n",
            "loss : 0.292\n",
            "epoch : 0\n",
            "loss : 0.312\n",
            "epoch : 0\n",
            "loss : 0.268\n",
            "epoch : 0\n",
            "loss : 0.261\n",
            "epoch : 0\n",
            "loss : 0.384\n",
            "epoch : 0\n",
            "loss : 0.319\n",
            "epoch : 0\n",
            "loss : 0.232\n",
            "epoch : 0\n",
            "loss : 0.382\n",
            "epoch : 0\n",
            "loss : 0.191\n",
            "epoch : 0\n",
            "loss : 0.218\n",
            "epoch : 0\n",
            "loss : 0.434\n",
            "epoch : 0\n",
            "loss : 0.223\n",
            "epoch : 0\n",
            "loss : 0.385\n",
            "epoch : 0\n",
            "loss : 0.307\n",
            "epoch : 0\n",
            "loss : 0.333\n",
            "epoch : 0\n",
            "loss : 0.376\n",
            "epoch : 0\n",
            "loss : 0.244\n",
            "epoch : 0\n",
            "loss : 0.265\n",
            "epoch : 0\n",
            "loss : 0.304\n",
            "epoch : 0\n",
            "loss : 0.361\n",
            "epoch : 0\n",
            "loss : 0.297\n",
            "epoch : 0\n",
            "loss : 0.435\n",
            "epoch : 0\n",
            "loss : 0.305\n",
            "epoch : 0\n",
            "loss : 0.248\n",
            "epoch : 0\n",
            "loss : 0.239\n",
            "epoch : 0\n",
            "loss : 0.314\n",
            "epoch : 0\n",
            "loss : 0.227\n",
            "epoch : 0\n",
            "loss : 0.160\n",
            "epoch : 0\n",
            "loss : 0.489\n",
            "epoch : 0\n",
            "loss : 0.315\n",
            "epoch : 0\n",
            "loss : 0.295\n",
            "epoch : 0\n",
            "loss : 0.201\n",
            "epoch : 0\n",
            "loss : 0.226\n",
            "epoch : 0\n",
            "loss : 0.274\n",
            "epoch : 0\n",
            "loss : 0.216\n",
            "epoch : 0\n",
            "loss : 0.320\n",
            "epoch : 0\n",
            "loss : 0.272\n",
            "epoch : 0\n",
            "loss : 0.225\n",
            "epoch : 0\n",
            "loss : 0.332\n",
            "epoch : 0\n",
            "loss : 0.266\n",
            "epoch : 0\n",
            "loss : 0.437\n",
            "epoch : 0\n",
            "loss : 0.364\n",
            "epoch : 0\n",
            "loss : 0.217\n",
            "epoch : 0\n",
            "loss : 0.285\n",
            "epoch : 0\n",
            "loss : 0.253\n",
            "epoch : 0\n",
            "loss : 0.431\n",
            "epoch : 0\n",
            "loss : 0.338\n",
            "epoch : 0\n",
            "loss : 0.436\n",
            "epoch : 0\n",
            "loss : 0.373\n",
            "epoch : 0\n",
            "loss : 0.221\n",
            "epoch : 0\n",
            "loss : 0.191\n",
            "epoch : 0\n",
            "loss : 0.299\n",
            "epoch : 0\n",
            "loss : 0.300\n",
            "epoch : 0\n",
            "loss : 0.248\n",
            "epoch : 0\n",
            "loss : 0.232\n",
            "epoch : 0\n",
            "loss : 0.414\n",
            "epoch : 0\n",
            "loss : 0.340\n",
            "epoch : 0\n",
            "loss : 0.274\n",
            "epoch : 0\n",
            "loss : 0.232\n",
            "epoch : 0\n",
            "loss : 0.240\n",
            "epoch : 0\n",
            "loss : 0.319\n",
            "epoch : 0\n",
            "loss : 0.147\n",
            "epoch : 0\n",
            "loss : 0.216\n",
            "epoch : 0\n",
            "loss : 0.229\n",
            "epoch : 0\n",
            "loss : 0.239\n",
            "epoch : 0\n",
            "loss : 0.158\n",
            "epoch : 0\n",
            "loss : 0.372\n",
            "epoch : 0\n",
            "loss : 0.225\n",
            "epoch : 0\n",
            "loss : 0.231\n",
            "epoch : 0\n",
            "loss : 0.249\n",
            "epoch : 0\n",
            "loss : 0.207\n",
            "epoch : 0\n",
            "loss : 0.406\n",
            "epoch : 0\n",
            "loss : 0.292\n",
            "epoch : 0\n",
            "loss : 0.203\n",
            "epoch : 0\n",
            "loss : 0.214\n",
            "epoch : 0\n",
            "loss : 0.240\n",
            "epoch : 0\n",
            "loss : 0.204\n",
            "epoch : 0\n",
            "loss : 0.245\n",
            "epoch : 0\n",
            "loss : 0.332\n",
            "epoch : 0\n",
            "loss : 0.277\n",
            "epoch : 0\n",
            "loss : 0.306\n",
            "epoch : 0\n",
            "loss : 0.212\n",
            "epoch : 0\n",
            "loss : 0.317\n",
            "epoch : 0\n",
            "loss : 0.221\n",
            "epoch : 0\n",
            "loss : 0.354\n",
            "epoch : 0\n",
            "loss : 0.215\n",
            "epoch : 0\n",
            "loss : 0.271\n",
            "epoch : 0\n",
            "loss : 0.165\n",
            "epoch : 0\n",
            "loss : 0.174\n",
            "epoch : 0\n",
            "loss : 0.301\n",
            "epoch : 0\n",
            "loss : 0.363\n",
            "epoch : 0\n",
            "loss : 0.300\n",
            "epoch : 0\n",
            "loss : 0.226\n",
            "epoch : 0\n",
            "loss : 0.300\n",
            "epoch : 0\n",
            "loss : 0.313\n",
            "epoch : 0\n",
            "loss : 0.312\n",
            "epoch : 0\n",
            "loss : 0.509\n",
            "epoch : 0\n",
            "loss : 0.339\n",
            "epoch : 0\n",
            "loss : 0.303\n",
            "epoch : 0\n",
            "loss : 0.246\n",
            "epoch : 0\n",
            "loss : 0.223\n",
            "epoch : 0\n",
            "loss : 0.362\n",
            "epoch : 0\n",
            "loss : 0.329\n",
            "epoch : 0\n",
            "loss : 0.293\n",
            "epoch : 0\n",
            "loss : 0.306\n",
            "epoch : 0\n",
            "loss : 0.465\n",
            "epoch : 0\n",
            "loss : 0.219\n",
            "epoch : 0\n",
            "loss : 0.276\n",
            "epoch : 0\n",
            "loss : 0.324\n",
            "epoch : 0\n",
            "loss : 0.406\n",
            "epoch : 0\n",
            "loss : 0.252\n",
            "epoch : 0\n",
            "loss : 0.169\n",
            "epoch : 0\n",
            "loss : 0.333\n",
            "epoch : 0\n",
            "loss : 0.160\n",
            "epoch : 0\n",
            "loss : 0.248\n",
            "epoch : 0\n",
            "loss : 0.308\n",
            "epoch : 0\n",
            "loss : 0.236\n",
            "epoch : 0\n",
            "loss : 0.213\n",
            "epoch : 0\n",
            "loss : 0.187\n",
            "epoch : 0\n",
            "loss : 0.424\n",
            "epoch : 0\n",
            "loss : 0.188\n",
            "epoch : 0\n",
            "loss : 0.281\n",
            "epoch : 0\n",
            "loss : 0.234\n",
            "epoch : 0\n",
            "loss : 0.270\n",
            "epoch : 0\n",
            "loss : 0.211\n",
            "epoch : 0\n",
            "loss : 0.403\n",
            "epoch : 0\n",
            "loss : 0.428\n",
            "epoch : 0\n",
            "loss : 0.276\n",
            "epoch : 0\n",
            "loss : 0.259\n",
            "epoch : 0\n",
            "loss : 0.209\n",
            "epoch : 0\n",
            "loss : 0.246\n",
            "epoch : 0\n",
            "loss : 0.225\n",
            "epoch : 0\n",
            "loss : 0.269\n",
            "epoch : 0\n",
            "loss : 0.363\n",
            "epoch : 0\n",
            "loss : 0.214\n",
            "epoch : 0\n",
            "loss : 0.300\n",
            "epoch : 0\n",
            "loss : 0.275\n",
            "epoch : 0\n",
            "loss : 0.389\n",
            "epoch : 0\n",
            "loss : 0.167\n",
            "epoch : 0\n",
            "loss : 0.226\n",
            "epoch : 0\n",
            "loss : 0.216\n",
            "epoch : 0\n",
            "loss : 0.239\n",
            "epoch : 0\n",
            "loss : 0.516\n",
            "epoch : 0\n",
            "loss : 0.245\n",
            "epoch : 0\n",
            "loss : 0.185\n",
            "epoch : 0\n",
            "loss : 0.272\n",
            "epoch : 0\n",
            "loss : 0.278\n",
            "epoch : 0\n",
            "loss : 0.181\n",
            "epoch : 0\n",
            "loss : 0.163\n",
            "epoch : 0\n",
            "loss : 0.209\n",
            "epoch : 0\n",
            "loss : 0.301\n",
            "epoch : 0\n",
            "loss : 0.200\n",
            "epoch : 0\n",
            "loss : 0.214\n",
            "epoch : 0\n",
            "loss : 0.207\n",
            "epoch : 0\n",
            "loss : 0.165\n",
            "epoch : 0\n",
            "loss : 0.222\n",
            "epoch : 0\n",
            "loss : 0.384\n",
            "epoch : 0\n",
            "loss : 0.107\n",
            "epoch : 0\n",
            "loss : 0.274\n",
            "epoch : 0\n",
            "loss : 0.141\n",
            "epoch : 0\n",
            "loss : 0.220\n",
            "epoch : 0\n",
            "loss : 0.386\n",
            "epoch : 0\n",
            "loss : 0.230\n",
            "epoch : 0\n",
            "loss : 0.159\n",
            "epoch : 0\n",
            "loss : 0.203\n",
            "epoch : 0\n",
            "loss : 0.234\n",
            "epoch : 0\n",
            "loss : 0.257\n",
            "epoch : 0\n",
            "loss : 0.322\n",
            "epoch : 0\n",
            "loss : 0.141\n",
            "epoch : 0\n",
            "loss : 0.173\n",
            "epoch : 0\n",
            "loss : 0.205\n",
            "epoch : 0\n",
            "loss : 0.207\n",
            "epoch : 0\n",
            "loss : 0.154\n",
            "epoch : 0\n",
            "loss : 0.361\n",
            "epoch : 0\n",
            "loss : 0.169\n",
            "epoch : 0\n",
            "loss : 0.299\n",
            "epoch : 0\n",
            "loss : 0.362\n",
            "epoch : 0\n",
            "loss : 0.247\n",
            "epoch : 0\n",
            "loss : 0.229\n",
            "epoch : 0\n",
            "loss : 0.137\n",
            "epoch : 0\n",
            "loss : 0.394\n",
            "epoch : 0\n",
            "loss : 0.284\n",
            "epoch : 0\n",
            "loss : 0.190\n",
            "epoch : 0\n",
            "loss : 0.215\n",
            "epoch : 0\n",
            "loss : 0.212\n",
            "epoch : 0\n",
            "loss : 0.287\n",
            "epoch : 0\n",
            "loss : 0.246\n",
            "epoch : 0\n",
            "loss : 0.370\n",
            "epoch : 0\n",
            "loss : 0.187\n",
            "epoch : 0\n",
            "loss : 0.320\n",
            "epoch : 0\n",
            "loss : 0.236\n",
            "epoch : 0\n",
            "loss : 0.264\n",
            "epoch : 0\n",
            "loss : 0.373\n",
            "epoch : 0\n",
            "loss : 0.248\n",
            "epoch : 0\n",
            "loss : 0.234\n",
            "epoch : 0\n",
            "loss : 0.255\n",
            "epoch : 0\n",
            "loss : 0.140\n",
            "epoch : 0\n",
            "loss : 0.303\n",
            "epoch : 0\n",
            "loss : 0.301\n",
            "epoch : 0\n",
            "loss : 0.262\n",
            "epoch : 0\n",
            "loss : 0.208\n",
            "epoch : 0\n",
            "loss : 0.243\n",
            "epoch : 0\n",
            "loss : 0.190\n",
            "epoch : 0\n",
            "loss : 0.261\n",
            "epoch : 0\n",
            "loss : 0.260\n",
            "epoch : 0\n",
            "loss : 0.194\n",
            "epoch : 0\n",
            "loss : 0.296\n",
            "epoch : 0\n",
            "loss : 0.301\n",
            "epoch : 0\n",
            "loss : 0.215\n",
            "epoch : 0\n",
            "loss : 0.225\n",
            "epoch : 0\n",
            "loss : 0.317\n",
            "epoch : 0\n",
            "loss : 0.275\n",
            "epoch : 0\n",
            "loss : 0.550\n",
            "epoch : 0\n",
            "loss : 0.108\n",
            "epoch : 0\n",
            "loss : 0.237\n",
            "epoch : 0\n",
            "loss : 0.176\n",
            "epoch : 0\n",
            "loss : 0.238\n",
            "epoch : 0\n",
            "loss : 0.246\n",
            "epoch : 0\n",
            "loss : 0.362\n",
            "epoch : 0\n",
            "loss : 0.235\n",
            "epoch : 0\n",
            "loss : 0.182\n",
            "epoch : 0\n",
            "loss : 0.277\n",
            "epoch : 0\n",
            "loss : 0.218\n",
            "epoch : 0\n",
            "loss : 0.274\n",
            "epoch : 0\n",
            "loss : 0.244\n",
            "epoch : 0\n",
            "loss : 0.210\n",
            "epoch : 0\n",
            "loss : 0.370\n",
            "epoch : 0\n",
            "loss : 0.217\n",
            "epoch : 0\n",
            "loss : 0.125\n",
            "epoch : 0\n",
            "loss : 0.244\n",
            "epoch : 0\n",
            "loss : 0.310\n",
            "epoch : 0\n",
            "loss : 0.383\n",
            "epoch : 0\n",
            "loss : 0.275\n",
            "epoch : 0\n",
            "loss : 0.423\n",
            "epoch : 0\n",
            "loss : 0.300\n",
            "epoch : 0\n",
            "loss : 0.218\n",
            "epoch : 0\n",
            "loss : 0.185\n",
            "epoch : 0\n",
            "loss : 0.448\n",
            "epoch : 0\n",
            "loss : 0.263\n",
            "epoch : 0\n",
            "loss : 0.150\n",
            "epoch : 0\n",
            "loss : 0.253\n",
            "epoch : 0\n",
            "loss : 0.296\n",
            "epoch : 0\n",
            "loss : 0.178\n",
            "epoch : 0\n",
            "loss : 0.252\n",
            "epoch : 0\n",
            "loss : 0.304\n",
            "epoch : 0\n",
            "loss : 0.277\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 1/100 [00:08<14:18,  8.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "loss : 0.307\n",
            "epoch : 0\n",
            "loss : 0.263\n",
            "epoch : 0\n",
            "loss : 0.301\n",
            "epoch : 0\n",
            "loss : 0.256\n",
            "epoch : 0\n",
            "loss : 0.295\n",
            "epoch : 0\n",
            "loss : 0.332\n",
            "epoch : 0\n",
            "loss : 0.140\n",
            "epoch : 0\n",
            "loss : 0.336\n",
            "epoch : 0\n",
            "loss : 0.312\n",
            "epoch : 0\n",
            "loss : 0.374\n",
            "epoch : 0\n",
            "loss : 0.157\n",
            "epoch : 0\n",
            "loss : 0.173\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 2/100 [00:20<16:57, 10.39s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[168], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m----> 3\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_samples\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mb_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# (100,1,28,28) -> (100, 28*28)\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb_samples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#print(b_samples.shape)\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# (100) -> (100,1)\u001b[39;49;00m\n",
            "File \u001b[1;32mc:\\Users\\Jorge\\Desktop\\MASTER_IA\\TORCH\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\Jorge\\Desktop\\MASTER_IA\\TORCH\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\Jorge\\Desktop\\MASTER_IA\\TORCH\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Jorge\\Desktop\\MASTER_IA\\TORCH\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\Jorge\\Desktop\\MASTER_IA\\TORCH\\env\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:139\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[0;32m    132\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m        index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m        tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for epoch in tqdm.tqdm(range(epochs)):\n",
        "  for i ,(b_samples ,b_labels) in enumerate(train_loader):\n",
        "    # (100,1,28,28) -> (100, 28*28)\n",
        "    b_samples = b_samples.view(100,28*28).to(device)\n",
        "    #print(b_samples.shape)\n",
        "    # (100) -> (100,1)\n",
        "    b_labels = b_labels.to(device)\n",
        "    \n",
        "    # forward now is == to calling the model\n",
        "    \n",
        "    y_pred = model(b_samples)\n",
        "    #print(y_pred.shape) # (100,10)\n",
        "\n",
        "    # compute loss for all the batch\n",
        "    l   = loss(input = y_pred, target = b_labels) # b_labels debe ser (100) pq crossentropyloss en torch solo necesita el indice de la label no el vector de dimendion 10 entero tipo one hot\n",
        "\n",
        "    # compute the gradients for each w_i : dloss/dw_i (usa grafo computacional y almacena ,en el tensor .grad asociado a cada parametro, el valor del gradiente que luego usa el algoritmo de optimizacion)\n",
        "    l.backward()\n",
        "\n",
        "    # paso del optimizer (en este caso un simple SGD) equivale a lo que haciamos a mano de w += -lr*w.grad\n",
        "    optimizer.step()\n",
        "    \n",
        "    # restore or eliminate the grads inside parameter.grad tensor for next iteration\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  print(f\"epoch : {epoch}\")\n",
        "  print(f\"loss : {l.item():.3f}\")\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyNsbbhcOxR5",
        "outputId": "d344168f-8c5b-45c1-b866-8a359f4ad548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2])\n",
            "tensor([[-1.3433,  0.4192],\n",
            "        [ 0.3304,  0.9006]])\n",
            "torch.Size([2, 2])\n",
            "tensor([1., 1.])\n",
            "tensor([[0.1465, 0.8535],\n",
            "        [0.3612, 0.6388]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(2,2)\n",
        "y = torch.randn(2,2)\n",
        "\n",
        "print(x.shape)\n",
        "print(x)\n",
        "x_soft_max = x.softmax(dim = -1)\n",
        "\n",
        "\n",
        "print(x_soft_max.shape)\n",
        "print(x_soft_max.sum(dim = 1))\n",
        "print(x_soft_max)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VCjpi0oiL9g",
        "outputId": "82ec7283-dfb7-4fdc-810c-d2c10d398540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2425, -0.6660],\n",
            "        [ 0.4344, -0.5432]])\n",
            "tensor([[ 0.0457,  1.4203],\n",
            "        [ 0.3156, -1.1674]])\n",
            "tensor([[ 0.0111, -0.9460],\n",
            "        [ 0.1371,  0.6342]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(2,2)\n",
        "y = torch.randn(2,2)\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "print(x * y )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUo6snO-ivUr",
        "outputId": "583d06ad-cb10-4341-bc94-dc84755ceb05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1991,  1.1219],\n",
            "        [-0.1516,  1.2511]])\n"
          ]
        }
      ],
      "source": [
        "print(x  @ y )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR2omUf6SKDv",
        "outputId": "375a404b-050b-400c-a16b-306317fd8063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.2999)\n",
            "tensor(0.7001)\n",
            "tensor(1.0000)\n"
          ]
        }
      ],
      "source": [
        "suma = 0\n",
        "for e in x_soft_max[:,0]:\n",
        "  print(e)\n",
        "  suma += e\n",
        "print(suma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nKRFw4CTR9U",
        "outputId": "9fc254d9-ecc5-4619-e50f-d1aceb29dd70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.7001)\n",
            "tensor(0.2779)\n",
            "tensor(0.9779)\n"
          ]
        }
      ],
      "source": [
        "suma = 0\n",
        "for e in x_soft_max[1,:]:\n",
        "  print(e)\n",
        "  suma += e\n",
        "print(suma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXLp1WB3F_iI",
        "outputId": "08dad541-a023-4fe0-8394-8b900a6a429f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[ 0.5207, -1.2700,  0.9622, -1.4488],\n",
            "          [-0.3503, -1.8362, -0.2652,  1.5133],\n",
            "          [-0.5715, -1.1178, -0.0886, -0.9020]],\n",
            "\n",
            "         [[-0.2685,  1.1808, -0.2541,  0.6832],\n",
            "          [ 0.6094, -0.1029, -0.8680,  0.7459],\n",
            "          [ 0.1279, -0.4754,  0.0697, -1.5372]]]])\n",
            "tensor([[[[ 0.5207, -1.2700,  0.9622, -1.4488],\n",
            "          [-0.2685,  1.1808, -0.2541,  0.6832]],\n",
            "\n",
            "         [[-0.3503, -1.8362, -0.2652,  1.5133],\n",
            "          [ 0.6094, -0.1029, -0.8680,  0.7459]],\n",
            "\n",
            "         [[-0.5715, -1.1178, -0.0886, -0.9020],\n",
            "          [ 0.1279, -0.4754,  0.0697, -1.5372]]]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.randn(1, 2, 3, 4)\n",
        "a.size()\n",
        "print(a)\n",
        "b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
        "b.size()\n",
        "print(b)\n",
        "c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
        "c.size()\n",
        "torch.equal(b, c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW5MjPReZLhY"
      },
      "outputs": [],
      "source": [
        "# Red estilo pytorch\n",
        "class FFNN(torch.nn.Module):\n",
        "  def __init__(self, d0=300, d1=200, d2=300, d3 = 3):\n",
        "    super(FFNN, self).__init__()\n",
        "\n",
        "    # Definimos capas (automáticamente se registran como parametros)\n",
        "    self.fc1 = torch.nn.Linear(d0, d1, bias=True)\n",
        "    self.fc2 = torch.nn.Linear(d1, d2, bias=True)\n",
        "    self.fc3 = torch.nn.Linear(d2,  d3, bias=True)\n",
        "\n",
        "  # Computa la pasada hacia adelante\n",
        "  def forward(self, x):\n",
        "\n",
        "    u1 = self.fc1(x)\n",
        "    h1 = torch.tanh(u1)\n",
        "    u2 = self.fc2(h1)\n",
        "    h2 = torch.sigmoid(u2)\n",
        "    u3 = self.fc3(h2)\n",
        "    _sft_max_last = torch.nn.Softmax(dim = 1)\n",
        "    y_pred = _sft_max_last(u3)\n",
        "\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui-y_i-UR1Rz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class RandomDataSet(Dataset):\n",
        "  def __init__(self, N, f, k):\n",
        "    R_N_f = torch.rand(N,f)\n",
        "    self.X = torch.bernoulli(R_N_f)\n",
        "    R_N_1 = torch.rand(N,k)\n",
        "    #self.Y = torch.bernoulli(R_N_1)\n",
        "    _sftmax = torch.nn.Softmax(dim=1)\n",
        "    self.Y  = _sftmax(R_N_1)\n",
        "    self.num_features = f\n",
        "\n",
        "  # Debemos definir __len__ para retornar el tamaño del dataset\n",
        "  def __len__(self):\n",
        "    return self.X.size()[0]\n",
        "\n",
        "  # Debemos definir __getitem__ para retornar el i-ésimo\n",
        "  # ejemplo en nuestro dataset.\n",
        "  def __getitem__(self, i):\n",
        "    return self.X[i], self.Y[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh96-3s-ZjSL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def loop_FFNN(dataset, batch_size, d1, d2, d3, lr,\n",
        "                 epochs, run_in_GPU=True, reports_every=1,\n",
        "                 cheq_grad=False):\n",
        "\n",
        "  # Define un tipo para los tensores según si correrá en la GPU o no\n",
        "  device = 'cuda' if run_in_GPU else 'cpu'\n",
        "\n",
        "  # d0 es la cantidad de features\n",
        "  d0 = dataset.num_features\n",
        "\n",
        "  # Crea la red\n",
        "  red = FFNN(d0, d1, d2, d3)\n",
        "\n",
        "  # Pasa la red al dispositivo elegido\n",
        "  red.to(device)\n",
        "\n",
        "  # Muestra la cantidad de parámetros\n",
        "  print('Red:', red)\n",
        "\n",
        "  # Crea un dataloader desde el dataset\n",
        "  data = DataLoader(dataset, batch_size, shuffle=True)\n",
        "\n",
        "  # Crea un optimizador para el descenso de gradiente\n",
        "  optimizador = torch.optim.SGD(red.parameters(), lr)\n",
        "\n",
        "  # Define una perdida\n",
        "  #perdida = torch.nn.BCELoss()\n",
        "  perdida = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  # Comienza el entrenamiento\n",
        "  tiempo_epochs = 0\n",
        "  for e in range(1,epochs+1):\n",
        "    inicio_epoch = time.time()\n",
        "\n",
        "    for (x,y) in data:\n",
        "      # Asegura de pasarlos a la GPU si fuera necesario\n",
        "      x, y = x.to(device), y.to(device)\n",
        "\n",
        "      # Computa la pasada hacia adelante (forward)\n",
        "      y_pred = red.forward(x)\n",
        "\n",
        "      # Computa la función de pérdida\n",
        "      L = perdida(y_pred,y)\n",
        "\n",
        "      # Computa los gradientes hacia atrás (backpropagation)\n",
        "      L.backward()\n",
        "\n",
        "      # Descenso de gradiente para actualizar los parámetros\n",
        "      optimizador.step()\n",
        "\n",
        "      # Limpia los gradientes\n",
        "      optimizador.zero_grad()\n",
        "\n",
        "    tiempo_epochs += time.time() - inicio_epoch\n",
        "\n",
        "    if e % reports_every == 0:\n",
        "      # Calcula la certeza de las predicciones sobre todo el conjunto\n",
        "      X = dataset.X.to(device)\n",
        "      Y = dataset.Y.to(device)\n",
        "\n",
        "      # Predice usando la red\n",
        "      Y_PRED = red.forward(X)\n",
        "\n",
        "      # Calcula la pérdida de todo el conjunto\n",
        "      L_total = perdida(Y_PRED, Y)\n",
        "\n",
        "      # Elige una clase dependiendo del valor de Y_PRED\n",
        "      Y_PRED_BIN = (Y_PRED >= 0.5).float()\n",
        "\n",
        "      correctos = torch.sum(Y_PRED_BIN == Y).item()\n",
        "      acc = (correctos / N) * 100\n",
        "\n",
        "      sys.stdout.write(\n",
        "            '\\rEpoch:{0:03d}'.format(e) + ' Acc:{0:.2f}%'.format(acc)\n",
        "            + ' Loss:{0:.4f}'.format(L_total)\n",
        "            + ' Tiempo/epoch:{0:.3f}s'.format(tiempo_epochs/e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJx5kndFBhgi"
      },
      "outputs": [],
      "source": [
        "N = 50000 # numero de ejemplos\n",
        "f = 300 # numero de features\n",
        "k = 3 # numero de clases\n",
        "\n",
        "dataset = RandomDataSet(N,f, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "XgaASdM0FnJ9",
        "outputId": "c90ee3d0-102b-4c15-de36-4afae4d4c658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Red: FFNN(\n",
            "  (fc1): Linear(in_features=300, out_features=500, bias=True)\n",
            "  (fc2): Linear(in_features=500, out_features=100, bias=True)\n",
            "  (fc3): Linear(in_features=100, out_features=5, bias=True)\n",
            ")\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "0D or 1D target tensor expected, multi-target not supported",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-ea9c0511e6a3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m loop_FFNN(dataset, batch_size=100, d1=500, d2=100, d3 = 5, epochs=epochs,\n\u001b[0m\u001b[1;32m      3\u001b[0m              run_in_GPU=True, lr=0.08)\n",
            "\u001b[0;32m<ipython-input-82-7c8d7fa68919>\u001b[0m in \u001b[0;36mloop_FFNN\u001b[0;34m(dataset, batch_size, d1, d2, d3, lr, epochs, run_in_GPU, reports_every, cheq_grad)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0;31m# Computa la función de pérdida\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m       \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperdida\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0;31m# Computa los gradientes hacia atrás (backpropagation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "loop_FFNN(dataset, batch_size=100, d1=500, d2=100, d3 = 3, epochs=epochs,\n",
        "             run_in_GPU=True, lr=0.08)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rr1JvHD8_VG9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
